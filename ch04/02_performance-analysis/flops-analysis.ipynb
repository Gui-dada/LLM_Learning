{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8409be5f898a419b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# FLOPS Analysis(FLOPS分析)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee031dd9fcffbdb8",
   "metadata": {},
   "source": [
    "- FLOP（每秒浮点运算）通过计算执行的浮点运算数量来衡量神经网络模型的计算复杂性\n",
    "- 高 FLOP 表示更密集的计算和能源消耗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "126248b729b09d37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T20:39:03.456246Z",
     "start_time": "2025-11-24T20:39:03.445736Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install -r requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced7167581332393",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T20:39:10.941363Z",
     "start_time": "2025-11-24T20:39:03.470044Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version:0.1.1-2209072238\n",
      "torch version:2.9.1+cu130\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "from transformers import GPTJModel\n",
    "\n",
    "pkgs=[\n",
    "    \"thop\",\n",
    "    \"torch\",\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version:{version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eda12455a69119",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 具有固定批量大小的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ef8d9a4a61ab5",
   "metadata": {},
   "source": [
    "- 仅向前传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f08b089d3f774198",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T20:39:20.196841Z",
     "start_time": "2025-11-24T20:39:11.244990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-small (124M)  :5.1e+11 FLOPS\n",
      "gpt-midium (355M) :1.4e+12 FLOPS\n",
      "gpt-large (744M)  :3.2e+12 FLOPS\n",
      "gpt-xl (1558M)    :6.4e+12 FLOPS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from thop import profile\n",
    "# pip install llms-from-scratch\n",
    "from llms_from_scratch.ch04 import GPTModel\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,  #词表大小\n",
    "    \"context_length\": 1024,  #上下文长度\n",
    "    \"drop_rate\": 0.0,  #丢失率\n",
    "    \"qkv_bias\": True,  #查询键值偏差\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt-midium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt-large (744M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(device)\n",
    "batch_size = 2\n",
    "input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device)\n",
    "\n",
    "for size in model_configs:\n",
    "    BASE_CONFIG.update(model_configs[size])\n",
    "\n",
    "    model = GPTModel(BASE_CONFIG).bfloat16()\n",
    "    model.to(device)\n",
    "\n",
    "    # MACS = 乘法累加运算\n",
    "    # MACS 通常计算为两次 FLOPS（一次乘法和一次累加）\n",
    "\n",
    "    macs,params = profile(model,inputs=(input_tensor,),verbose=False)\n",
    "    flops =2*macs\n",
    "    print(f\"{size:18}:{flops:.1e} FLOPS\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb26420b41fe9f1",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 具有自动批量大小查找功能的简单基准测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5c39c820f55b5",
   "metadata": {},
   "source": [
    "- 仅向前传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59660dd5871de108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T20:55:58.719050Z",
     "start_time": "2025-11-24T20:55:58.712539Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing gpt-small (124M) 处理中...\n",
      "Batch size 256: 6.5e+13 FLOPS\n",
      "Batch size 384: 9.7e+13 FLOPS\n",
      "Batch size 416: 1.1e+14 FLOPS\n",
      "Batch size 432: 1.1e+14 FLOPS\n",
      "Batch size 440: 1.1e+14 FLOPS\n",
      "Batch size 444: 1.1e+14 FLOPS\n",
      "Batch size 446: 1.1e+14 FLOPS\n",
      "Batch size 447: 1.1e+14 FLOPS\n",
      "\n",
      "Processing gpt-midium (355M) 处理中...\n",
      "Batch size 256: 1.9e+14 FLOPS\n",
      "Batch size 384: 2.8e+14 FLOPS\n",
      "\n",
      "Processing gpt-large (744M) 处理中...\n",
      "Batch size 256: 4.0e+14 FLOPS\n",
      "Batch size 264: 4.2e+14 FLOPS\n",
      "Batch size 268: 4.2e+14 FLOPS\n",
      "Batch size 270: 4.3e+14 FLOPS\n",
      "Batch size 271: 4.3e+14 FLOPS\n",
      "\n",
      "Processing gpt-xl (1558M) 处理中...\n",
      "Batch size 256: 8.2e+14 FLOPS\n"
     ]
    }
   ],
   "source": [
    "for size in model_configs:\n",
    "    print(f\"\\nProcessing {size} 处理中...\")\n",
    "    config = BASE_CONFIG.copy()\n",
    "    config.update(model_configs[size])\n",
    "\n",
    "    min_batch_size = 1\n",
    "    max_batch_size = None\n",
    "    max_possible_batch_size = 4096\n",
    "    while min_batch_size <= max_possible_batch_size:\n",
    "        batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
    "        try:\n",
    "            input_tensor = torch.randint(\n",
    "                0,config[\"vocab_size\"],\n",
    "                (batch_size,config[\"context_length\"]),\n",
    "                device=device\n",
    "            )\n",
    "            model = GPTModel(config).bfloat16().to(device)\n",
    "            # MACS = 乘法累加运算\n",
    "            # MACS 通常计算为两次 FLOPS（一次乘法和一次累加）\n",
    "            macs,params = profile(model,inputs=(input_tensor,),verbose=False)\n",
    "            flops =2*macs\n",
    "            print(f\"Batch size {batch_size}: {flops:.1e} FLOPS\")\n",
    "\n",
    "            #如果成功，请尝试更大的批量大小\n",
    "            min_batch_size = batch_size+1\n",
    "            max_batch_size = batch_size\n",
    "\n",
    "            #清理\n",
    "            del model,input_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                # 尝试较小的批量\n",
    "                max_possible_batch_size = batch_size-1\n",
    "\n",
    "                #清理\n",
    "                try:\n",
    "                    del model,input_tensor\n",
    "                    torch.cuda.empty_cache()\n",
    "                except NameError:\n",
    "                    pass\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f18f582d0db5a5",
   "metadata": {},
   "source": [
    "## GPU性能基准测试结果 - RTX 5070 Ti (16GB)\n",
    "\n",
    "| 模型        | 参数量     | 最大批量 | 峰值FLOPS  | 状态    |\n",
    "|------------|---------|------|-----------|---------|\n",
    "| GPT-Small   | 124M    | 462  | 1.2e+14   | ✅ 稳定 |\n",
    "| GPT-Medium  | 355M    | ≥384 | 2.8e+14   | ✅ 良好 |\n",
    "| GPT-Large   | 744M    | -    | -         | ⚠️ 边界 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503de32d84e35136",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 具有自动批量大小查找和模型 FLOP 利用率 (MFU) 的基准"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a118b5f374975c9",
   "metadata": {},
   "source": [
    "- [PaLM 论文]中的模型FLOP利用率(MFU)解释(https://arxiv.org/abs/2204.02311)\n",
    "\n",
    "> 我们提出了一种新的效率指标，该指标与实现无关，并且允许对系统效率进行更清晰的比较，称为模型浮点运算利用率（MFU）。这是观察到的吞吐量（每秒Tokens数）相对于以峰值 FLOP 运行的系统的理论最大吞吐量的比率。至关重要的是，“理论最大”吞吐量仅考虑计算前向+后向传递所需的操作，而不考虑重新物化。\n",
    "\n",
    "\n",
    "$$\\text{MFU} = \\frac{\\text{每秒观察到的Tokens数}}{\\text{每秒理论最大Tokens数}}$$\n",
    "\n",
    "哪里\n",
    "\n",
    "$$\\text{每秒理论最大Tokens数} = \\frac{\\text{每秒最大 FLOPs}}{\\text{每个Tokens的总 FLOPs}}$$\n",
    "\n",
    "和\n",
    "\n",
    "$$\\text{每秒Tokens数} = \\frac{\\text{批量大小} \\times \\text{序列长度}}{\\text{总时间}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcc539bb6d961cc",
   "metadata": {},
   "source": [
    "- 向前和向后传递"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2bcaf3697e6e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T20:52:50.719451Z",
     "start_time": "2025-11-24T20:52:50.702413Z"
    }
   },
   "outputs": [],
   "source": [
    "# Theoretical max flops per second provided by the GPU manufacturer\n",
    "\n",
    "flops_per_second = {\n",
    "    # https://www.techpowerup.com/gpu-specs/h100-pcie-80-gb.c3899\n",
    "    \"H100\": {\n",
    "        torch.float32: 51.22e12,  # 51.22 TFLOPs for FP32 on NVIDIA H100\n",
    "        torch.float16: 204.9e12,  # 204.9 TFLOPs for FP16 on NVIDIA H100\n",
    "        torch.bfloat16: 204.9e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/l4.c4091\n",
    "    \"L4\": {\n",
    "        torch.float32: 30.29e12,  # 30.29 TFLOPs for FP32 on NVIDIA L4\n",
    "        torch.float16: 30.29e12,  # 30.29 TFLOPs for FP16 on NVIDIA L4\n",
    "        torch.bfloat16: 30.29e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/tesla-t4.c3316\n",
    "    \"T4\": {\n",
    "        torch.float32: 8.1e12,  # 8.1 TFLOPs for FP32 on NVIDIA T4\n",
    "        torch.float16: 65.13e12,  # 65.13 TFLOPs for FP16 on NVIDIA T4\n",
    "        torch.bfloat16: 65.13e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/a10g.c3798\n",
    "    \"A10G\": {\n",
    "        torch.float32: 31.52e12,  # 31.52 TFLOPs for FP32 on NVIDIA A10G\n",
    "        torch.float16: 31.52e12,  # 31.52 TFLOPs for FP16 on NVIDIA A10G\n",
    "        torch.bfloat16: 31.52e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623\n",
    "    \"A100\": {\n",
    "        torch.float32: 19.49e12,  # 19.49 TFLOPs for FP32 on NVIDIA A100\n",
    "        torch.float16: 77.97e12,  # 77.97 TFLOPs for FP16 on NVIDIA A100\n",
    "        torch.bfloat16: 77.97e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3080.c3621\n",
    "    \"RTX 3080\": {\n",
    "        torch.float32: 29.77e12,  # 29.77 TFLOPs for FP32 on NVIDIA RTX 3080\n",
    "        torch.float16: 29.77e12,  # 29.77 TFLOPs for FP16 on NVIDIA RTX 3080\n",
    "        torch.bfloat16: 29.77e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-3090.c3622\n",
    "    \"RTX 3090\": {\n",
    "        torch.float32: 35.58e12,  # 35.58 TFLOPs for FP32 on NVIDIA RTX 3090\n",
    "        torch.float16: 35.58e12,  # 35.58 TFLOPs for FP16 on NVIDIA RTX 3090\n",
    "        torch.bfloat16: 35.58e12\n",
    "    },\n",
    "    # https://www.techpowerup.com/gpu-specs/geforce-rtx-5070-ti.c4243\n",
    "    \"RTX 5070 Ti\":{\n",
    "        torch.float32:43.94e12, # 43.94 TFLOPs for FP32 on NVIDIA RTX_5070Ti\n",
    "        torch.float16:43.94e12, # 43.94 TFLOPs for FP32 on NVIDIA RTX_5070Ti\n",
    "        torch.bfloat16:43.94e12\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fdfb77e2ad90ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T20:52:50.750992Z",
     "start_time": "2025-11-24T20:52:50.726450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Model: RTX 5070 Ti\n",
      "\n",
      "Processing gpt-small (124M) 处理中...\n",
      "  Batch size 32: Tokens/sec: 901.45, MFU: 0.0152\n",
      "\n",
      "Processing gpt-midium (355M) 处理中...\n",
      "  Batch size 1: Tokens/sec: 293.08, MFU: 0.0141\n",
      "\n",
      "Processing gpt-large (744M) 处理中...\n",
      "  Batch size 8: Tokens/sec: 176.39, MFU: 0.0186\n",
      "  Batch size 10: Tokens/sec: 121.51, MFU: 0.0128\n",
      "\n",
      "Processing gpt-xl (1558M) 处理中...\n",
      "  Batch size 4: Tokens/sec: 67.54, MFU: 0.0143\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def get_gpu_model(flops_per_second_dict):\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "    # print(f\"Device name: {device_name}\")\n",
    "    for model in flops_per_second_dict.keys():\n",
    "        if model in device_name:\n",
    "            return model\n",
    "    return \"Unknown\"  # 如果没有找到匹配的型号则默认\n",
    "\n",
    "\n",
    "gpu_model = get_gpu_model(flops_per_second)\n",
    "print(\"GPU Model:\", gpu_model)\n",
    "\n",
    "if gpu_model != \"Unknown\":\n",
    "\n",
    "    for size in model_configs:\n",
    "        print(f\"\\nProcessing {size} 处理中...\")\n",
    "        config = BASE_CONFIG.copy()\n",
    "        config.update(model_configs[size])\n",
    "\n",
    "        min_batch_size = 1\n",
    "        max_batch_size = None\n",
    "        max_possible_batch_size = 4096\n",
    "\n",
    "        while min_batch_size <= max_possible_batch_size:\n",
    "            batch_size = (min_batch_size + max_possible_batch_size) // 2\n",
    "            try:\n",
    "                input_tensor = torch.randint(\n",
    "                    0, config[\"vocab_size\"],\n",
    "                    (batch_size, config[\"context_length\"]),\n",
    "                    device=device\n",
    "                )\n",
    "\n",
    "                model = GPTModel(config).bfloat16().to(device)\n",
    "                model.train()\n",
    "\n",
    "                # 开始计时\n",
    "                torch.cuda.synchronize()\n",
    "                start_time = time.time()\n",
    "\n",
    "                # 向前和向后传递\n",
    "                output = model(input_tensor)\n",
    "                loss = output.sum()  # 计算虚拟损失\n",
    "                loss.backward()\n",
    "\n",
    "                # 结束计时\n",
    "                torch.cuda.synchronize()\n",
    "                end_time = time.time()\n",
    "\n",
    "                total_time_seconds = end_time - start_time\n",
    "\n",
    "                # 计算前向传播的 FLOPs\n",
    "                macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "                flops_forward = 2 * macs  # 假设 1 个 MAC 等于 2 个 FLOP\n",
    "\n",
    "                # 估计向后传递的 FLOP（通常是前向 FLOP 的 2 倍）\n",
    "                flops_backward = 2 * flops_forward\n",
    "\n",
    "                # 前向 + 后向传递的总 FLOP 次数\n",
    "                total_flops = flops_forward + flops_backward  #或者total_flops = flops_forward * 3\n",
    "\n",
    "                data_type = next(model.parameters()).dtype\n",
    "                max_flops_per_second = flops_per_second[gpu_model].get(data_type, 0)\n",
    "\n",
    "                # 每秒计算tokens数\n",
    "                tokens_processed = batch_size * config[\"context_length\"]\n",
    "                tokens_per_second = tokens_processed / total_time_seconds\n",
    "\n",
    "                # 计算每个token的 FLOPs\n",
    "                flops_per_token = total_flops / tokens_processed\n",
    "\n",
    "                # 计算每秒理论最大tokens数\n",
    "                if flops_per_token > 0:\n",
    "                    theoretical_max_tokens_per_second = max_flops_per_second / flops_per_token\n",
    "                else:\n",
    "                    theoretical_max_tokens_per_second = 0  # 避免除0\n",
    "\n",
    "                # 计算MFU\n",
    "                if theoretical_max_tokens_per_second > 0:\n",
    "                    mfu = tokens_per_second / theoretical_max_tokens_per_second\n",
    "                else:\n",
    "                    mfu = 0  # 避免除0\n",
    "\n",
    "                print(f\"  Batch size {batch_size}: Tokens/sec: {tokens_per_second:.2f}, MFU: {mfu:.4f}\")\n",
    "\n",
    "                # 如果成功，请尝试更大的批量大小\n",
    "                min_batch_size = batch_size + 1\n",
    "                max_batch_size = batch_size\n",
    "\n",
    "                # 清理\n",
    "                del model, input_tensor, output, loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    # 尝试较小的批量\n",
    "                    max_possible_batch_size = batch_size - 1\n",
    "\n",
    "                    # 清除\n",
    "                    try:\n",
    "                        del model, input_tensor\n",
    "                        torch.cuda.empty_cache()\n",
    "                    except NameError:\n",
    "                        pass\n",
    "                else:\n",
    "                    raise e\n",
    "\n",
    "else:\n",
    "    print(\"Unknown GPU model. Please update the flops_per_second dictionary with your GPU information.\")\n",
    "    print(\"未知 GPU 型号。请使用您的 GPU 信息更新 flops_per_second 字典。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
