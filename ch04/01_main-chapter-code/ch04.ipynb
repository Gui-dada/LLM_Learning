{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd871c86-9bec-48a5-b227-10bd51dff6e3",
   "metadata": {},
   "source": [
    "# 第 4 章：从头开始实现 GPT 模型以生成文本"
   ]
  },
  {
   "cell_type": "code",
   "id": "af02ea40-a1f5-43a0-be8e-d8d7060f5685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.080073Z",
     "start_time": "2025-11-24T16:52:28.063558Z"
    }
   },
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "from torch.nn import LayerNorm\n",
    "from torch.special import logit\n",
    "\n",
    "print(\"matplotlib version:\", version(\"matplotlib\"))\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.10.7\n",
      "torch version: 2.9.1+cu130\n",
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "execution_count": 609
  },
  {
   "cell_type": "markdown",
   "id": "b50c471c-6b58-444b-bf5d-74ac2868ab54",
   "metadata": {},
   "source": [
    "- 在本章中，我们实现了类似GPT的LLM架构；下一章将重点培训该LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54147153-9b4f-4e58-b642-84d8b59ca1da",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/01.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf265cb5-362b-43dd-b863-a715064f5944",
   "metadata": {},
   "source": [
    "## 4.1 编码 LLM 架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfce55-f611-4473-b060-5989d1ce0fbc",
   "metadata": {},
   "source": [
    "- 第 1 章讨论了 GPT 和 Llama 等模型，它们按顺序生成单词，并且基于原始 Transformer 架构的解码器部分\n",
    "- 因此，这些LLMs通常被称为“类似解码器”的LLM\n",
    "- 与传统的深度学习模型相比，LLM 更大，主要是由于其参数数量庞大，而不是代码量\n",
    "- 我们会看到很多元素在LLM的架构中重复出现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaddc56e-8ab7-46bb-bc29-4d864a6cb286",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/02.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb50141e-11cd-4b98-aa6d-e31d03d66880",
   "metadata": {},
   "source": [
    "- 在前面的章节中，为了便于说明，我们对token输入和输出使用了较小的嵌入尺寸，确保它们适合单个页面\n",
    "- 在本章中，我们考虑类似于小型 GPT-2 模型的嵌入和模型大小\n",
    "- 我们将专门编码最小 GPT-2 模型（1.24 亿个参数）的架构，如 Radford 等人的[语言模型是无监督多任务学习者](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 中所述（请注意，初始报告将其列为 117M 个参数，但这后来在模型权重存储库）\n",
    "- 第 6 章将展示如何将预训练权重加载到我们的实现中，这将与 345、762 和 15.42 亿个参数的模型大小兼容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d307f688-7c01-45cb-9be3-de67fc15b0bb",
   "metadata": {},
   "source": [
    "- 1.24亿参数GPT-2模型的配置详细信息包括："
   ]
  },
  {
   "cell_type": "code",
   "id": "60c1ba8b-005f-428c-85b3-f809af0493b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.095108Z",
     "start_time": "2025-11-24T16:52:28.090581Z"
    }
   },
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # 词汇量\n",
    "    \"context_length\": 1024,  # 上下文长度\n",
    "    \"emb_dim\": 768,  # 嵌入尺寸\n",
    "    \"n_heads\": 12,  # 注意力头数量\n",
    "    \"n_layers\": 12,  # 层数\n",
    "    \"drop_rate\": 0.1,  # 丢弃率\n",
    "    \"qkv_bias\": False  # 查询键值偏差\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 610
  },
  {
   "cell_type": "markdown",
   "id": "f0087d85-bd70-4a26-a4ed-e22c0dc78be4",
   "metadata": {},
   "source": [
    "- 我们使用短变量名以避免以后出现长行代码\n",
    "- `\"vocab_size\"` 表示词汇量大小为 50,257 个单词，由第 2 章中讨论的 BPE 分词器支持\n",
    "- `\"context_length\"` 表示模型的最大输入标记计数，由第 2 章中介绍的位置嵌入启用\n",
    "- `\"emb_dim\"` 是 token 输入的嵌入大小，将每个输入 token 转换为 768 维向量\n",
    "- `\"n_heads\"` 是第 3 章中实现的多头注意力机制中注意力头的数量\n",
    "- `\"n_layers\"` 是模型中transformer块的数量，我们将在接下来的部分中实现\n",
    "- `\"drop_rate\"` 是 dropout 机制的强度，在第 3 章中讨论； 0.1 表示在训练期间丢弃 10% 的隐藏单元以减轻过度拟合\n",
    "- `\"qkv_bias\"` 决定多头注意力机制中的 `Linear` 层（来自第 3 章）在计算查询 (Q)、键 (K) 和值 (V) 张量时是否应包含偏差向量；我们将禁用此选项，这是现代LLM的标准做法；然而，稍后当将 OpenAI 中的预训练 GPT-2 权重加载到第 5 章的重新实现中时，我们将重新讨论这一点\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae2c7f-1ea0-4fdd-8042-a54e64638b4b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/03.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "id": "42d42b06-793e-4379-aa28-0db7473361d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.110108Z",
     "start_time": "2025-11-24T16:52:28.101101Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # 为TransformerBlock添加一个占位符\n",
    "        self.trf_block = nn.Sequential(\n",
    "            *[DummyTransformmerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        # 使用占位符LayerNorm\n",
    "        self.final_norm = nn.LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_emb = self.tok_emb(in_idx)\n",
    "        pos_emb = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_block(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformmerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 一个简单的占位符\n",
    "\n",
    "    def forward(self, x):\n",
    "        #这个块什么都不做，只是返回它的输入。\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # 这里的参数只是模仿LayerNorm接口。\n",
    "\n",
    "    def forward(self, x):\n",
    "        #这一层什么都不做，只是返回它的输入。\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 611
  },
  {
   "cell_type": "markdown",
   "id": "7db21331-e8d2-4682-9374-66d36ff61f16",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/04.webp?123\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "id": "a1426edc-35e5-4670-9b49-2b3cc1be0f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.125665Z",
     "start_time": "2025-11-24T16:52:28.114618Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "execution_count": 612
  },
  {
   "cell_type": "code",
   "id": "845d1ed6-9e43-42cf-b078-b3da6b1e92b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.324852Z",
     "start_time": "2025-11-24T16:52:28.135674Z"
    }
   },
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.7867,  0.2203, -0.4508,  ..., -0.9936, -0.1412, -0.2999],\n",
      "         [-0.0788,  0.3004, -0.2935,  ...,  0.1583,  0.8917,  0.8230],\n",
      "         [ 0.3708,  1.1126, -0.3226,  ...,  0.8023, -0.0038,  0.3935],\n",
      "         [ 0.0636,  1.0572, -0.2507,  ...,  0.7542, -0.0750, -0.6896]],\n",
      "\n",
      "        [[-0.7208,  0.1351, -0.6014,  ..., -1.0272,  0.1729, -0.2920],\n",
      "         [-0.5938,  0.4453, -0.0059,  ...,  0.3414,  0.0572,  1.0986],\n",
      "         [ 0.2675,  0.8407, -0.4476,  ..., -0.0181, -0.1090,  0.2541],\n",
      "         [-0.1035, -0.5901, -0.3932,  ...,  1.4022, -0.3188,  0.1304]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 613
  },
  {
   "cell_type": "markdown",
   "id": "d21222cf-edfa-4a3b-87c0-8c9d7b2026d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**注意**\n",
    "\n",
    "- 如果您在 Windows上运行此代码，上面的结果值可能如下所示：\n",
    "\n",
    "``` \n",
    "Output shape: torch.Size([2, 4, 50257])\n",
    "tensor([[[-0.7867,  0.2203, -0.4508,  ..., -0.9936, -0.1412, -0.2999],\n",
    "         [-0.0788,  0.3004, -0.2935,  ...,  0.1583,  0.8917,  0.8230],\n",
    "         [ 0.3708,  1.1126, -0.3226,  ...,  0.8023, -0.0038,  0.3935],\n",
    "         [ 0.0636,  1.0572, -0.2507,  ...,  0.7542, -0.0750, -0.6896]],\n",
    "\n",
    "        [[-0.7208,  0.1351, -0.6014,  ..., -1.0272,  0.1729, -0.2920],\n",
    "         [-0.5938,  0.4453, -0.0059,  ...,  0.3414,  0.0572,  1.0986],\n",
    "         [ 0.2675,  0.8407, -0.4476,  ..., -0.0181, -0.1090,  0.2541],\n",
    "         [-0.1035, -0.5901, -0.3932,  ...,  1.4022, -0.3188,  0.1304]]],\n",
    "       grad_fn=<UnsafeViewBackward0>)\n",
    "```\n",
    "\n",
    "- 由于这些只是随机数，因此不必担心，您可以毫无问题地继续本章的其余部分\n",
    "- 造成这种差异的一个可能原因是“nn.Dropout”在不同操作系统上的行为不同，具体取决于 PyTorch 的编译方式，如 [此处在 PyTorch 问题跟踪器上](https://github.com/pytorch/pytorch/issues/121595) 所讨论的\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d751a2-7ea2-47fa-9da8-ab45e3a1f113",
   "metadata": {},
   "source": [
    "## 4.2 使用层归一化对激活进行归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d60b6-41e1-441e-ac64-d292a569b1bd",
   "metadata": {},
   "source": [
    "- 层归一化，也称为 LayerNorm（[Ba et al. 2016](https://arxiv.org/abs/1607.06450)），将神经网络层的激活集中在平均值 0 周围，并将其方差归一化为 1\n",
    "- 这可以稳定训练并能够更快地收敛到有效重量\n",
    "- 层归一化在变压器块内的多头注意模块之前和之后应用，我们将在稍后实现；它也应用于最终输出层之前"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1475e8ef-36a4-462d-a878-6932a093b596",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/05.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cba398-f590-478d-9684-359d00814e87",
   "metadata": {},
   "source": [
    "- 让我们通过简单的神经网络层传递一个小输入样本来看看层归一化是如何工作的："
   ]
  },
  {
   "cell_type": "code",
   "id": "008bc2f7-e6d9-4cbe-97d7-215e03070ef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.339875Z",
     "start_time": "2025-11-24T16:52:28.328857Z"
    }
   },
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "#创建 2 个训练示例，每个示例有 5 个维度（特征）\n",
    "batch_example = torch.randn(2, 5)\n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "execution_count": 614
  },
  {
   "cell_type": "markdown",
   "id": "b9f21fc0-3024-4454-89b7-c0053b186ae7",
   "metadata": {},
   "source": [
    "- 让我们计算上面 2 个输入中每一个的均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "id": "0ea5d385-1ec6-4935-b258-23a387766558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.354897Z",
     "start_time": "2025-11-24T16:52:28.344387Z"
    }
   },
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 615
  },
  {
   "cell_type": "markdown",
   "id": "257e4ed1-ebcb-4b76-b422-faa38ca6aac1",
   "metadata": {},
   "source": [
    "- 标准化独立地应用于两个输入（行）；使用 dim=-1 将计算应用于最后一个维度（在本例中为特征维度）而不是行维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104601e-0eb2-46d7-b6bb-f68d45f9dd18",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/06.webp\" width=\"500px\">"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 减去平均值并除以方差（标准差）的平方根，使输入在列（特征）维度上的平均值为 0，方差为 1：",
   "id": "392b91448100529c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.370416Z",
     "start_time": "2025-11-24T16:52:28.359901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"归一化层输出：\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"平均值:\\n\", mean)\n",
    "print(\"方差:\\n\", var)"
   ],
   "id": "9704b8108cb31bc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "归一化层输出：\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "平均值:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "方差:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 616
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "-每个输入以0为中心，单位方差为1；为了提高可读性，我们可以禁用PyTorch的科学符号：",
   "id": "9a94011146223bd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.385431Z",
     "start_time": "2025-11-24T16:52:28.375926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"平均值:\\n\", mean)\n",
    "print(\"方差:\\n\", var)"
   ],
   "id": "c8298c8cc661f21f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均值:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "方差:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 617
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "-以上，我们对每个输入的特征进行了归一化\n",
    "-现在，使用相同的想法，我们可以实现一个`LayerNorm`类："
   ],
   "id": "3f609c594e42a5ef"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.400945Z",
     "start_time": "2025-11-24T16:52:28.391433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ],
   "id": "a4cc08a66a83d106",
   "outputs": [],
   "execution_count": 618
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**缩放和移动**\n",
    "\n",
    "- 请注意，除了通过减去平均值和除以方差来执行归一化之外，我们还添加了两个可训练的参数，一个`scale`和一个`shift`参数\n",
    "- 初始的`scale`（乘以1）和`shift`（加0）值没有任何影响；然而，`scale`和`shift`是可训练的参数，如果确定这样做会提高模型在训练任务中的表现，LLM会在训练期间自动调整这些参数\n",
    "-这允许模型学习适当的缩放和移动，最适合它正在处理的数据\n",
    "-注意，在计算方差的平方根之前，我们还添加了一个较小的值（`eps`）；这是为了避免方差为0时的除零错误\n",
    "\n",
    "**偏方差**\n",
    "- 在上面的方差计算中，设置`unbiased=False`意味着使用公式 $\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$ 计算方差，其中n是样本量（这里是特征或列的数量）；该公式不包括贝塞尔校正（在分母中使用`n-1`），因此提供了对方差的有偏估计\n",
    "-对于llm，其中嵌入维数n非常大，使用n和n-1的区别\n",
    "可以忽略不计\n",
    "- 然而，GPT-2在规范化层中使用有偏差的方差进行训练，这就是为什么我们也采用了这种设置，因为我们将在后面的章节中加载预训练的权重\n",
    "\n",
    "- 现在让我们在实践中尝试`LayerNorm`："
   ],
   "id": "751339f49f586ad9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.415968Z",
     "start_time": "2025-11-24T16:52:28.404457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ],
   "id": "daa4f578c5b67370",
   "outputs": [],
   "execution_count": 619
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.431482Z",
     "start_time": "2025-11-24T16:52:28.420970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"平均值:\\n\", mean)\n",
    "print(\"方差:\\n\", var)"
   ],
   "id": "ad6ae52b0f68123c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均值:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "方差:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "execution_count": 620
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/07.webp\" width=\"500px\">",
   "id": "e1f0f69c49f3a1af"
  },
  {
   "cell_type": "markdown",
   "id": "26fbf359-312e-45c8-bdcb-44be81bbd140",
   "metadata": {},
   "source": [
    "## 4.3 使用 GELU 激活实现前馈网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcff52-a572-4a83-b270-2400569047ed",
   "metadata": {},
   "source": [
    "- 在本节中，我们实现一个小型神经网络子模块，用作LLMs中transformer块的一部分\n",
    "- 我们从激活函数开始\n",
    "- 在深度学习中，ReLU（整流线性单元）激活函数因其简单性和有效性而在各种神经网络架构中得到广泛使用\n",
    "- 在LLM中，除了传统的ReLU之外，还使用了各种其他类型的激活函数；两个著名的例子是 GELU（高斯误差线性单元）和 SwiGLU（Swish 门控线性单元）\n",
    "- GELU 和 SwiGLU 是更复杂、更平滑的激活函数，分别包含高斯和 sigmoid 门控线性单元，为深度学习模型提供更好的性能，这与 ReLU 更简单的分段线性函数不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77c3364-488a-47eb-85c6-81908dd32849",
   "metadata": {},
   "source": [
    "- GELU（[Hendrycks and Gimpel 2016](https://arxiv.org/abs/1606.08415)）可以通过多种方式实现；确切的版本定义为 GELU(x)=x⋅Φ(x)，其中 Φ(x) 是标准高斯分布的累积分布函数。\n",
    "- 在实践中，通常会实现计算成本较低的近似值： $\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)\n",
    "$（原始 GPT-2 模型也是用这个近似值进行训练的）"
   ]
  },
  {
   "cell_type": "code",
   "id": "897a6e5e-d758-4087-9103-49ada479ec23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.447009Z",
     "start_time": "2025-11-24T16:52:28.436499Z"
    }
   },
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ],
   "outputs": [],
   "execution_count": 621
  },
  {
   "cell_type": "code",
   "id": "b481ee1f-6260-43e9-ae76-5d6a823d8d11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.554396Z",
     "start_time": "2025-11-24T16:52:28.453513Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# 一些样本数据\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 622
  },
  {
   "cell_type": "markdown",
   "id": "2d66b8f2-62b5-4770-8ee7-7ca86aa9c0c0",
   "metadata": {},
   "source": [
    "- 正如我们所看到的，ReLU是一个分段线性函数，如果输入为正则直接输出；否则，输出零\n",
    "- GELU 是一个平滑的非线性函数，近似于 ReLU，但对于负值具有非零梯度（除了大约 -0.75）\n",
    "\n",
    "- 接下来，让我们实现小型神经网络模块`FeedForward`，稍后我们将在 LLM 的transformer块中使用它："
   ]
  },
  {
   "cell_type": "code",
   "id": "b44cb0da-b85f-4507-aa5d-bd841b785b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.569918Z",
     "start_time": "2025-11-24T16:52:28.563399Z"
    }
   },
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ],
   "outputs": [],
   "execution_count": 623
  },
  {
   "cell_type": "code",
   "id": "c4df6af5-0718-4dab-b40e-0d5c438dd417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.585115Z",
     "start_time": "2025-11-24T16:52:28.573599Z"
    }
   },
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "execution_count": 624
  },
  {
   "cell_type": "code",
   "id": "e3ad8aea-fd63-40d3-b221-09e092aec924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.616201Z",
     "start_time": "2025-11-24T16:52:28.593620Z"
    }
   },
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# 输入形状：[batch_size， num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "\n",
    "print(out.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "execution_count": 625
  },
  {
   "cell_type": "markdown",
   "id": "f5491bef-94a1-46d1-ab46-77ef6930bf34",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/10.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda88b2c-c912-4b7f-9898-ee945f88e4b6",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/11.webp\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01dde43d-2d11-46a0-9970-3ef0e01be3f6",
   "metadata": {},
   "source": [
    "## 4.4 添加快捷连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f432c8-94b4-4d76-960d-544831887ccb",
   "metadata": {},
   "source": [
    "- 接下来，我们来谈谈快捷连接背后的概念，也称为跳过或剩余连接\n",
    "- 最初，在计算机视觉的深度网络（残差网络）中提出了快捷连接，以减轻梯度消失问题\n",
    "- 快捷连接为梯度流经网络创建了一条替代的较短路径\n",
    "- 这是通过将一层的输出添加到后面一层的输出来实现的，通常跳过中间的一层或多层\n",
    "- 让我们用一个小示例网络来说明这个想法：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/12.webp?123\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e86385-df80-4b9c-921d-66f8ab49c468",
   "metadata": {},
   "source": [
    "- 在代码中，它看起来像这样："
   ]
  },
  {
   "cell_type": "code",
   "id": "ffb0fda5-67d3-439b-8e1a-7db293e7d929",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.631710Z",
     "start_time": "2025-11-24T16:52:28.623704Z"
    }
   },
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # 计算当前层的输出\n",
    "            layer_output = layer(x)\n",
    "            # 检查是否可以应用快捷方式\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # 前向传递\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    # 根据目标的接近程度计算损失和输出是\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    # 向后传递计算梯度\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # 打印权重的平均绝对梯度\n",
    "            print(f\"{name} 梯度平均值为 {param.grad.abs().mean().item()}\")"
   ],
   "outputs": [],
   "execution_count": 626
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T12:13:21.361903Z",
     "start_time": "2025-11-24T12:13:21.354668Z"
    }
   },
   "cell_type": "markdown",
   "source": "- 让我们首先打印梯度值**没有**快捷方式连接：",
   "id": "cfe6f9c398b42673"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.647228Z",
     "start_time": "2025-11-24T16:52:28.636719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ],
   "id": "3b425e29ea307db1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight 梯度平均值为 0.0011121002025902271\n",
      "layers.1.0.weight 梯度平均值为 0.001235906733199954\n",
      "layers.2.0.weight 梯度平均值为 0.001773848314769566\n",
      "layers.3.0.weight 梯度平均值为 0.003020398784428835\n"
     ]
    }
   ],
   "execution_count": 627
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 接下来，让我们**用**快捷方式连接打印渐变值：",
   "id": "4f25100cfe74351b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.662741Z",
     "start_time": "2025-11-24T16:52:28.650230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ],
   "id": "e5061b9d4237a752",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight 梯度平均值为 0.25530001521110535\n",
      "layers.1.0.weight 梯度平均值为 0.21116767823696136\n",
      "layers.2.0.weight 梯度平均值为 0.29930680990219116\n",
      "layers.3.0.weight 梯度平均值为 0.2527087330818176\n"
     ]
    }
   ],
   "execution_count": 628
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 正如我们根据上面的输出所看到的，快捷连接防止梯度在早期层中消失（朝向“layer.0”）\n",
    "- 接下来，当我们实现变压器块时，我们将使用这种快捷连接的概念"
   ],
   "id": "bb4b49ef2efd5907"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.5 连接transformer块中的注意力层和线性层",
   "id": "3a2f5bfa6312014c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 在本节中，我们现在将前面的概念结合到所谓的transformer块中\n",
    "- transformer块将前一章中的因果多头注意力模块与线性层（我们在前面部分中实现的前馈神经网络）相结合\n",
    "- 此外，transformer块还使用了dropout和shortcut连接"
   ],
   "id": "46a2c51aeadedb2a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.677759Z",
     "start_time": "2025-11-24T16:52:28.668250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.dropout_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)  #Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.att(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut  #添加原始输入\n",
    "\n",
    "        # 前馈块的快捷连接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.dropout_shortcut(x)\n",
    "        x = x + shortcut  #添加原始输入\n",
    "        return x"
   ],
   "id": "14de4762ef4bbccc",
   "outputs": [],
   "execution_count": 629
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/13.webp?1\" width=\"500px\">",
   "id": "8e1b0a3515ca7e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 假设我们有2个输入样本，每个样本有6个标记，其中每个标记是一个768维的嵌入向量；然后，这个变压器块应用自关注，然后是线性层，以产生类似大小的输出\n",
    "- 你可以把输出看作是我们在前一章讨论的上下文向量的增强版本"
   ],
   "id": "e756cd6c1392f9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.708284Z",
     "start_time": "2025-11-24T16:52:28.683263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  #Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape 输入形状:\", x.shape)\n",
    "print(\"Output shape 输出形式:\", output.shape)"
   ],
   "id": "608bb3370c0aa625",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape 输入形状: torch.Size([2, 4, 768])\n",
      "Output shape 输出形式: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "execution_count": 630
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/14.webp?1\" width=\"500px\">",
   "id": "f3a2728cd1402626"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:24:03.096371Z",
     "start_time": "2025-11-24T15:24:03.082350Z"
    }
   },
   "cell_type": "markdown",
   "source": "## 4.6编写GPT模型",
   "id": "e4dcb8313196edf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:24:37.433774Z",
     "start_time": "2025-11-24T15:24:37.408750Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "- 我们差不多完成了：现在让我们将transformer 块插入到我们在本章开头编写的体系结构中，以便我们获得可用的GPT体系结构\n",
    "- 注意transformer 块重复多次；对于最小的124M GPT-2型号，我们重复12次："
   ],
   "id": "5d3e8985f40a0f52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/15.webp\" width=\"500px\">",
   "id": "a5d0ad90d65d8b04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 相应的代码实现，其中`cfg[\"n_layers\"] = 12`:",
   "id": "4538fc60e1a30c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:28.723298Z",
     "start_time": "2025-11-24T16:52:28.711285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ],
   "id": "1466354f651ee02c",
   "outputs": [],
   "execution_count": 631
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 使用124M参数模型的配置，我们现在可以用随机初始权重实例化这个GPT模型，如下所示：",
   "id": "f9f883f241c3905a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.120624Z",
     "start_time": "2025-11-24T16:52:28.728304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch 输入批处理:\\n\", batch)\n",
    "print(\"\\nOutput shape 输出形状:\", out.shape)\n",
    "print(out)"
   ],
   "id": "90b8562646f73bb7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch 输入批处理:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape 输出形状: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 632
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 我们将在下一章训练这个模型\n",
    "- 不过，关于它的尺寸，我们之前称它为124M参数模型我们可以再核对一下这个数字："
   ],
   "id": "86bec3ae67d3de47"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.135665Z",
     "start_time": "2025-11-24T16:52:29.130132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters 参数总数: {total_params:,}\")"
   ],
   "id": "5406ca0fdd8e6668",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters 参数总数: 163,009,536\n"
     ]
    }
   ],
   "execution_count": 633
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:40:25.415589Z",
     "start_time": "2025-11-24T15:40:25.409079Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "- 正如我们在上面看到的，这个模型有 163M，而不是 124M 参数；为什么？\n",
    "- 在最初的 GPT-2 论文中，研究人员应用了权重绑定，这意味着他们重用了 token 嵌入层（`tok_emb`）作为输出层，这意味着设置 `self.out_head.weight = self.tok_emb.weight`\n",
    "- token嵌入层将 50,257 维单热编码输入令牌投影为 768 维嵌入表示\n",
    "- 输出层将 768 维嵌入投影回 50,257 维表示，以便我们可以将它们转换回单词（下一节将详细介绍）\n",
    "- 因此，嵌入层和输出层具有相同数量的权重参数，正如我们根据权重矩阵的形状所看到的那样\n",
    "- 但是，关于其大小的快速说明：我们之前将其称为 124M 参数模型；我们可以仔细检查这个数字，如下所示："
   ],
   "id": "34ab7cf23701f39c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.151174Z",
     "start_time": "2025-11-24T16:52:29.147174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Token embedding layer shape(Token嵌入层形状):\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape(输出层形状):\", model.out_head.weight.shape)"
   ],
   "id": "c4ad4cd8e9b0c592",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape(Token嵌入层形状): torch.Size([50257, 768])\n",
      "Output layer shape(输出层形状): torch.Size([50257, 768])\n"
     ]
    }
   ],
   "execution_count": 634
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 在最初的 GPT-2 论文中，研究人员重新使用了 token 嵌入矩阵作为输出矩阵\n",
    "- 相应地，如果我们减去输出层的参数数量，我们将得到一个124M参数的模型："
   ],
   "id": "62860e2c8346d0e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.166194Z",
     "start_time": "2025-11-24T16:52:29.161683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters consiering weight tying 考虑权重绑定的可训练参数数量: {total_params_gpt2:,}\")"
   ],
   "id": "70d2ecf4cb303e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters consiering weight tying 考虑权重绑定的可训练参数数量: 124,412,160\n"
     ]
    }
   ],
   "execution_count": 635
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T15:48:44.419189Z",
     "start_time": "2025-11-24T15:48:44.404167Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "- 在实践中，我发现在没有权重绑定的情况下训练模型更容易，这就是我们没有在这里实现它的原因\n",
    "- 但是，稍后当我们在第 5 章中加载预训练权重时，我们将重新审视并应用这个权重绑定的想法\n",
    "- 最后，我们可以如下计算模型的内存需求，这可以作为有用的参考点："
   ],
   "id": "b58a0faf743e4e43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.181700Z",
     "start_time": "2025-11-24T16:52:29.177700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 计算总大小（以字节为单位）（假设float32，每个参数4字节）\n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# 转换为兆字节\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ],
   "id": "cefe8bfafcdd1735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "execution_count": 636
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 练习：您可m以尝试以下其他配置，这些配置也在 [GPT-2 论文](https://scholar.google.com/itations?view_op=view_itation&hl=en&user=dOad5HoAAAAJ&itation_for_view=dOad5HoAAAAJ:YsMSGLbcyi4C) 中引用。\n",
    "-\n",
    "    - **GPT2-small** （我们已经实现的124M配置）:\n",
    "        - \"emb_dim\" = 768\n",
    "        - \"n_layers\" = 12\n",
    "        - \"n_heads\" = 12\n",
    "\n",
    "    - **GPT2-medium:**\n",
    "        - \"emb_dim\" = 1024\n",
    "        - \"n_layers\" = 24\n",
    "        - \"n_heads\" = 16\n",
    "\n",
    "    - **GPT2-large:**\n",
    "        - \"emb_dim\" = 1280\n",
    "        - \"n_layers\" = 36\n",
    "        - \"n_heads\" = 20\n",
    "\n",
    "    - **GPT2-XL:**\n",
    "        - \"emb_dim\" = 1600\n",
    "        - \"n_layers\" = 48\n",
    "        - \"n_heads\" = 25"
   ],
   "id": "56ba71bbccdd9ea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.7 生成文本",
   "id": "62d33c99cb4c8ed4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 像我们上面实现的 GPT 模型这样的 LLM 用于一次生成一个单词",
   "id": "845e94fa21cfa215"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/16.webp\" width=\"500px\">",
   "id": "666158e96f850e8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 下面的`generate_text_simple`函数实现了贪婪解码，这是一种简单快速的生成文本的方法\n",
    "- 在贪婪解码中，在每一步中，模型都会选择概率最高的单词（或标记）作为其下一个输出（最高的 logit 对应于最高的概率，因此从技术上讲，我们甚至不必显式计算 softmax 函数）\n",
    "- 在下一章中，我们将实现更高级的 `generate_text` 函数\n",
    "- 下图描述了 GPT 模型如何在给定输入上下文的情况下生成下一个单词标记"
   ],
   "id": "ab2d530969f115c4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/17.webp\" width=\"700px\">",
   "id": "9196d8da60cfe4a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.196723Z",
     "start_time": "2025-11-24T16:52:29.189209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx 是当前上下文中的 (batch, n_tokens) 索引数组\n",
    "    for _ in range(max_new_tokens):\n",
    "        # 如果当前上下文超出了支持的上下文大小，则裁剪当前上下文\n",
    "        # 例如，如果 LLM 仅支持 5 个令牌，并且上下文大小为 10\n",
    "        # 那么只有最后 5 个标记被用作上下文\n",
    "\n",
    "        idx_cond = idx[:,-context_size:]\n",
    "        # 获取预测结果\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        # 只关注最后一个时间步\n",
    "        # (batch, n_tokens, vocab_size) 变为 (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # 应用softmax来获取概率\n",
    "        probas = torch.softmax(logits, dim=-1)\n",
    "        # 获取概率值最高的词汇条目的idx\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "        # 将采样索引附加到运行序列中\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx"
   ],
   "id": "9253b633d9851231",
   "outputs": [],
   "execution_count": 637
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 上面的 `generate_text_simple` 实现了一个迭代过程，一次创建一个token\n",
    "\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch04_compressed/18.webp\" width=\"700px\">"
   ],
   "id": "adc67aef23156ed1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 让我们准备一个输入示例：",
   "id": "d3c1475f23245b53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.212231Z",
     "start_time": "2025-11-24T16:52:29.203723Z"
    }
   },
   "cell_type": "code",
   "source": [
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded 编码:\",encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape 编码张量形式:\",encoded_tensor.shape)"
   ],
   "id": "677414bcc4f51df1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded 编码: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape 编码张量形式: torch.Size([1, 4])\n"
     ]
    }
   ],
   "execution_count": 638
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.334874Z",
     "start_time": "2025-11-24T16:52:29.218741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output 输出:\",out)\n",
    "print(\"Output  输出长度:\",len(out[0]))"
   ],
   "id": "9412abc634d23d6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output 输出: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output  输出长度: 10\n"
     ]
    }
   ],
   "execution_count": 639
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 删除批量维度并转换回文本：",
   "id": "d3b31f3e85bef683"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T16:52:29.350388Z",
     "start_time": "2025-11-24T16:52:29.346389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ],
   "id": "1cd5df8f5fc9118",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "execution_count": 640
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 请注意，模型未经训练；因此上面的随机输出文本\n",
    "- 我们将在下一章训练模型"
   ],
   "id": "8fe76c866d3f5822"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 总结和要点\n",
    "\n",
    "- 请参阅 [./gpt.py](./gpt.py) 脚本，这是一个独立的脚本，其中包含我们在此 Jupyter 笔记本中实现的 GPT 模型"
   ],
   "id": "78c3d71a24ede5b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
