{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6f771c-165d-44d9-9a4d-b31a38735765",
   "metadata": {},
   "source": [
    "# 了解 PyTorch 缓冲区"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f347233f-c444-4e30-abbb-58ba9f76d0e6",
   "metadata": {},
   "source": [
    "本质上，PyTorch 缓冲区是与 PyTorch 模块或模型相关的张量属性，类似于参数，但与参数不同的是，缓冲区在训练期间不会更新。\n",
    "PyTorch 中的缓冲区在处理 GPU 计算时特别有用，因为它们需要与模型参数一起在设备之间传输（例如从 CPU 到 GPU）。与参数不同，缓冲区不需要梯度计算，但它们仍然需要位于正确的设备上以确保所有计算正确执行。\n",
    "在第 3 章中，我们通过`self.register_buffer`使用 PyTorch 缓冲区，书中仅对此进行了简要说明。由于概念和目的尚不清楚，因此此代码笔记本通过实践示例提供了更长的解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1c51e-0f37-4043-9a15-cea601abf3ae",
   "metadata": {},
   "source": [
    "## 没有缓冲区的示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedfd20-0cdb-4254-bbce-7d821a3da494",
   "metadata": {},
   "source": [
    "假设我们有以下代码，它基于第 3 章中的代码。该版本已修改为排除缓冲区。它实现了 LLM 中使用的因果自注意力机制："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dd28150-bd5c-4fab-8014-3e1633fa11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithoutBuffers(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd4ccb6-c321-4c23-bf3e-0aec9f4c8e19",
   "metadata": {},
   "source": [
    "我们可以在一些示例数据上初始化并运行该模块，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8e6ee39-0a66-49a0-9bd1-19c212d1ef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "context_length = batch.shape[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554d6c8-f79e-4698-877f-737209874c44",
   "metadata": {},
   "source": [
    "到目前为止，一切都进展顺利。\n",
    "\n",
    "然而，在训练 LLM 时，我们通常使用 GPU 来加速该过程。因此，我们将`CausalAttentionWithoutBuffers`模块转移到 GPU 设备上。\n",
    "\n",
    "请注意，此操作需要代码在配备GPU的环境中运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e953c7-878a-434a-aa3a-1bbe6c2d6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine has GPU: True\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "has_cuda = torch.cuda.is_available()\n",
    "has_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(\"Machine has GPU:\", has_cuda or has_mps)\n",
    "\n",
    "if has_mps:\n",
    "    device = torch.device(\"mps\")   # Apple Silicon GPU (Metal)\n",
    "elif has_cuda:\n",
    "    device = torch.device(\"cuda\")  # NVIDIA GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "batch = batch.to(device)\n",
    "ca_without_buffer = ca_without_buffer.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b0209c-34da-499f-bbad-16c828946e13",
   "metadata": {},
   "source": [
    "现在，让我们再次运行代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a828e6e1-fbc0-4639-bee2-513c6df00e4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected self and mask to be on the same device, but got mask on cpu and self on cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 2\u001b[0m     context_vecs \u001b[38;5;241m=\u001b[39m \u001b[43mca_without_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(context_vecs)\n",
      "File \u001b[1;32mE:\\CODING\\LLM_Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\CODING\\LLM_Learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 23\u001b[0m, in \u001b[0;36mCausalAttentionWithoutBuffers.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_value(x)\n\u001b[0;32m     22\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m@\u001b[39m keys\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[43mattn_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\n\u001b[0;32m     26\u001b[0m     attn_scores \u001b[38;5;241m/\u001b[39m keys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     28\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weights)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected self and mask to be on the same device, but got mask on cpu and self on cuda:0"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734b2643-f34d-4c08-9c37-28f4d9487ee4",
   "metadata": {},
   "source": [
    "运行代码导致错误。发生了什么？看起来我们尝试了 GPU 上的张量和 CPU 上的张量之间的矩阵乘法。但我们把模块移到了 GPU 上！？\n",
    "\n",
    "让我们仔细检查一些张量的设备位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af658df9-8b66-43b3-acb6-5d7088ccdfe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.device: cuda:0\n",
      "mask.device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"W_query.device:\", ca_without_buffer.W_query.weight.device)\n",
    "print(\"mask.device:\", ca_without_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5021c75e-f4b9-4856-96d8-fc7c14d75af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of ca_without_buffer.W_query.weight:  <class 'torch.nn.parameter.Parameter'>\n",
      "type of ca_without_buffer.mask :  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(\"type of ca_without_buffer.W_query.weight: \",type(ca_without_buffer.W_query.weight))\n",
    "print(\"type of ca_without_buffer.mask : \",type(ca_without_buffer.mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5aa81d8-791e-48f9-8532-e53c25f26445",
   "metadata": {},
   "source": [
    "正如我们所看到的，`mask`没有移到 GPU 上。这是因为它不是像权重那样的 PyTorch 参数（例如`W_query.weight`）。\n",
    "\n",
    "这意味着我们必须通过 `.to(\"cuda\")` 手动将其移动到 GPU："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12ff6df8-73b8-40d3-a3a6-7a679b81a650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ca_without_buffer.mask = ca_without_buffer.mask.to(device)\n",
    "print(\"mask.device:\", ca_without_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85428d28-16cc-43b5-b1d9-78486475f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_without_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d91cee-64c4-4332-8553-c7483d6158bb",
   "metadata": {},
   "source": [
    "这一次，成功了！\n",
    "\n",
    "然而，记住将各个张量移动到 GPU 可能很乏味。正如我们将在下一节中看到的，使用`register_buffer`将`mask`注册为缓冲区会更容易。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd14c8-94c3-452d-b8a9-24b4c294f4fb",
   "metadata": {},
   "source": [
    "## 一个带有缓冲区的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0934a621-c157-4075-84b8-0d0d14b8ebc3",
   "metadata": {},
   "source": [
    "现在让我们修改因果注意类以将因果`mask`注册为缓冲区："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b7aa18d-b2ca-42d2-b212-60c0b0742f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CausalAttentionWithBuffer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # 旧:\n",
    "        # self.mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "\n",
    "        # 新:\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe567f-45ef-46a9-a998-52e5dae8630c",
   "metadata": {},
   "source": [
    "现在，方便的是，如果我们将模块移动到 GPU，掩码也将位于 GPU 上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fac7163-249b-4625-9db5-d35f54a99b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query.device: cuda:0\n",
      "mask.device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
    "ca_with_buffer.to(device)\n",
    "\n",
    "print(\"W_query.device:\", ca_with_buffer.W_query.weight.device)\n",
    "print(\"mask.device:\", ca_with_buffer.mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2186ee7c-39a9-4df2-8a2f-bd17184732c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5684,  0.5063],\n",
      "         [-0.5388,  0.6447],\n",
      "         [-0.5242,  0.6954],\n",
      "         [-0.4578,  0.6471],\n",
      "         [-0.4006,  0.5921],\n",
      "         [-0.3997,  0.5971]],\n",
      "\n",
      "        [[-0.5684,  0.5063],\n",
      "         [-0.5388,  0.6447],\n",
      "         [-0.5242,  0.6954],\n",
      "         [-0.4578,  0.6471],\n",
      "         [-0.4006,  0.5921],\n",
      "         [-0.3997,  0.5971]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    context_vecs = ca_with_buffer(batch)\n",
    "\n",
    "print(context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb5d013-9213-4f2a-8424-b3af80cc54a1",
   "metadata": {},
   "source": [
    "正如我们在上面所看到的，将张量注册为缓冲区可以让我们的方式变得更加轻松：我们不必记住手动将张量移动到 GPU 等目标设备。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6baf0-85f4-481e-be6c-228ce5a31d70",
   "metadata": {},
   "source": [
    "## 缓冲区和 `state_dict`(`状态字典`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ef6f3-7a6b-4f84-a8d7-a6c567363605",
   "metadata": {},
   "source": [
    "- 与常规张量相比，PyTorch 缓冲区的另一个优点是它们包含在模型的`state_dict`中\n",
    "- 例如，考虑没有缓冲区的因果注意对象的`state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "851de0c9-027c-46d9-8ab5-48ca0f7fe031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('W_query.weight',\n",
       "              tensor([[-0.2354,  0.0191, -0.2867],\n",
       "                      [ 0.2177, -0.4919,  0.4232]], device='cuda:0')),\n",
       "             ('W_key.weight',\n",
       "              tensor([[-0.4196, -0.4590, -0.3648],\n",
       "                      [ 0.2615, -0.2133,  0.2161]], device='cuda:0')),\n",
       "             ('W_value.weight',\n",
       "              tensor([[-0.4900, -0.3503, -0.2120],\n",
       "                      [-0.1135, -0.4404,  0.3780]], device='cuda:0'))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_without_buffer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc9d3a5-cf22-44ae-864f-306fc708668a",
   "metadata": {},
   "source": [
    "- 掩码不包含在上面的`state_dict`中\n",
    "- 然而，由于将其注册为缓冲区，掩码*包含在下面的`state_dict`中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b79f810-dc22-49d0-ba14-2ffda8bd6f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('mask',\n",
       "              tensor([[0., 1., 1., 1., 1., 1.],\n",
       "                      [0., 0., 1., 1., 1., 1.],\n",
       "                      [0., 0., 0., 1., 1., 1.],\n",
       "                      [0., 0., 0., 0., 1., 1.],\n",
       "                      [0., 0., 0., 0., 0., 1.],\n",
       "                      [0., 0., 0., 0., 0., 0.]], device='cuda:0')),\n",
       "             ('W_query.weight',\n",
       "              tensor([[-0.0746, -0.5366, -0.3570],\n",
       "                      [ 0.4928,  0.0345, -0.4677]], device='cuda:0')),\n",
       "             ('W_key.weight',\n",
       "              tensor([[ 0.0911,  0.4770, -0.5456],\n",
       "                      [-0.3887, -0.2299,  0.0232]], device='cuda:0')),\n",
       "             ('W_value.weight',\n",
       "              tensor([[-0.1347, -0.0634, -0.5629],\n",
       "                      [ 0.2704,  0.5068,  0.3529]], device='cuda:0'))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_with_buffer.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f436d2f7-75de-49f9-9be9-c2982e70653b",
   "metadata": {},
   "source": [
    "- 例如，在保存和加载经过训练的 PyTorch 模型时，`state_dict`非常有用\n",
    "- 在这种特殊情况下，保存和加载`mask`可能不是非常有用，因为它在训练期间保持不变；因此，出于演示目的，我们假设它已被修改，其中所有`1`都更改为`2`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48641edc-a7e9-48a3-84bd-41251e2c4e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_with_buffer.mask[ca_with_buffer.mask ==1.] =2.\n",
    "ca_with_buffer.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee467ce8-bab7-400e-8f6b-9c8422c57475",
   "metadata": {},
   "source": [
    "- 然后，如果我们保存并加载模型，我们可以看到掩模已恢复为修改后的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc529efa-8c44-4da6-b467-23b00c23ee8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 2.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(ca_with_buffer.state_dict(), \"model.pth\")\n",
    "\n",
    "new_ca_with_buffer = CausalAttentionWithBuffer(d_in, d_out, context_length, 0.0)\n",
    "new_ca_with_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "new_ca_with_buffer.mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b934a54-7bee-4620-abc4-41499154ef39",
   "metadata": {},
   "source": [
    "- 如果我们不使用缓冲区，则事实并非如此："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e48730ec-825f-43ac-ad9b-5cb50d0e4bfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_without_buffer.mask[ca_without_buffer.mask == 1.] = 2.\n",
    "\n",
    "torch.save(ca_without_buffer.state_dict(), \"model.pth\")\n",
    "\n",
    "new_ca_without_buffer = CausalAttentionWithoutBuffers(d_in, d_out, context_length, 0.0)\n",
    "new_ca_without_buffer.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "new_ca_without_buffer.mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
