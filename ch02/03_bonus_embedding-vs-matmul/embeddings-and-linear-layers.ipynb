{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d9e9487-5433-4972-b1ab-48f1d2102919",
   "metadata": {},
   "source": [
    "# 理解嵌入层和线性层的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d2e30-34b7-41bb-a957-90a6e7254399",
   "metadata": {},
   "source": [
    "- PyTorch 中的嵌入层与执行矩阵乘法的线性层功能相同；我们使用嵌入层的原因是为了提高计算效率。\n",
    "- 将通过 PyTorch 中的代码示例逐步了解这种关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b43333c6-7761-458d-ae6b-efe0eae88acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:  2.9.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version: \", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea374322-c3e1-4ef2-b868-7f8e32736d94",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 使用 nn.Embedding(可训练的“词向量表”)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078ab83f-70fe-4376-9a01-43f651715403",
   "metadata": {},
   "source": [
    "nn.Embedding是`PyTorch中的一个常用模块，其主要作用是将输入的整数序列转换为密集向量表示`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e2a251-563b-4436-be62-b4d4d3eb6c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 1])\n",
      "tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# 假设我们有三个可训练的案例，它们可能代表 LLM 上下文中的标记 ID\n",
    "idx = torch.tensor([2,3,1])\n",
    "# 我们可以通过嵌入矩阵的行数量来决定最大的标记id为 ID+1\n",
    "# 如果最高令牌 ID 为 3，则我们需要 4 行，分别对应可能的令牌 ID：0、1、2、3。\n",
    "num_idx = max(idx)+1\n",
    "print(idx)\n",
    "print(num_idx\n",
    "# 期望的嵌入维度是一个超参数\n",
    "out_dim = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc4f81a-e3f2-499c-b781-59d4b7e7505f",
   "metadata": {},
   "source": [
    "- 让我们实现一个简单的嵌入层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76e8c6f5-2632-47a1-a927-a44529571178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用随机种子是为了保证结果的可复现性，因为嵌入层中的权重是用小的随机值初始化的\n",
    "torch.manual_seed(123)\n",
    "# 输出4行5列\n",
    "embedding = torch.nn.Embedding(num_idx,out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cf7587-fad1-4914-b39d-758757607175",
   "metadata": {},
   "source": [
    "- 我们还可以选择性地查看嵌入权重："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e23b7f0-1580-4211-abec-b47a3118ce30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.3035, -0.5880,  1.5810],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015],\n",
       "        [ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953]], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7c1770-5d0b-48e9-95c3-c3fec5e79396",
   "metadata": {},
   "source": [
    "- 然后我们可以使用嵌入层来获得 ID 为 1 的训练样本的向量表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0138bc2f-c494-4caa-b149-8b540115f7a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0a7edc-12e2-42c1-8e06-2b0d1c529c20",
   "metadata": {},
   "source": [
    "- 下面这张图展示了底层运行机制："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eefaf3-1d4e-4bf1-9afd-e93307bea7fa",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/1.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e962f6f-381d-4ce5-9cd9-9bcef773171d",
   "metadata": {},
   "source": [
    "- 类似地，我们可以使用嵌入层来获得 ID 为 2 的训练样本的向量表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cbdc7fb-24f7-4be7-b200-de0bac8c4a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(torch.tensor([2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9bbba9-bad8-479f-8da2-dbbef616362b",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/2.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fa51e-ca77-48fd-af66-816cc658d310",
   "metadata": {},
   "source": [
    "- 现在，让我们转换之前定义的所有训练样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc48ef64-0ad2-4bcf-8377-917210270f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = torch.tensor([2, 3, 1])\n",
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bad7f52-822c-4a13-a55d-bd2a7fb6dc01",
   "metadata": {},
   "source": [
    "- 其底层原理仍然是相同的查找概念："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d99495-5592-4977-8811-8127f6bc0bf8",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/3.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafb8c3-c811-4903-a2db-9a0570c936a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "&nbsp;\n",
    "\n",
    "## 使用nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84056cf3-86d5-456c-9a7c-c85abc7b0fb1",
   "metadata": {},
   "source": [
    "nn.Linear() 是`PyTorch 中定义全连接层（也称为线性层、全连接层或密集层）的函数`。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac77ccd-7209-4e4f-817b-090dbc33a9a1",
   "metadata": {},
   "source": [
    "- 现在，我们将证明上面的嵌入层与 PyTorch 中基于独热编码表示的 `nn.Linear` 层实现的功能完全相同。\n",
    "- 首先，让我们将 token ID 转换为独热编码表示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a93f0a-28fb-44aa-9045-f0d4648ea9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0],\n",
       "        [0, 0, 0, 1],\n",
       "        [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot(idx)\n",
    "onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f072d-39a6-4645-8ac3-38fa181e731a",
   "metadata": {},
   "source": [
    "- 下一步,初始化`Liner`层,该层执行矩阵乘法 $X W^\\top$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b5fe86f-8a32-42bf-9bbf-b70711a98def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2039,  0.0166, -0.2483,  0.1886],\n",
       "        [-0.4260,  0.3665, -0.3634, -0.3975],\n",
       "        [-0.3159,  0.2264, -0.1847,  0.1871],\n",
       "        [-0.4244, -0.3034, -0.1836, -0.0983],\n",
       "        [-0.3814,  0.3274, -0.1179,  0.1605]], requires_grad=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "liner = torch.nn.Linear(num_idx,out_dim,bias= False)\n",
    "liner.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c78a0d-e6bc-46a5-a95c-33dc51684048",
   "metadata": {},
   "source": [
    "- 请注意，PyTorch 中的线性层也使用较小的随机权重进行初始化；为了将其与上面的 `Embedding` 层直接比较，我们必须使用相同的较小随机权重，这就是为什么我们在这里重新赋值它们的原因："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe2a6baa-4fbf-4a5e-a6c5-15fe6e63ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "liner.weight = torch.nn.Parameter(embedding.weight.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0a2409-a017-4df5-b334-a4e74a7c5f1f",
   "metadata": {},
   "source": [
    "- 现在我们可以对输入的独热编码表示使用线性层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c81c434d-e1bc-49e7-af22-de2395d4a805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liner(onehot.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc20cc3d-7ef8-4045-b154-c0d9dfbedea8",
   "metadata": {},
   "source": [
    "- 正如我们所看到的，这与我们使用嵌入层时得到的结果完全相同："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2be66646-663f-4216-9dd5-9aada8bfc27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6957, -1.8061, -1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096, -0.4076,  0.7953],\n",
       "        [ 1.3010,  1.2753, -0.2010, -0.1606, -0.4015]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1606235a-39a6-4fde-8bea-9555a7424816",
   "metadata": {},
   "source": [
    "- 其底层执行的是以下针对第一个训练样本的标记 ID 的计算："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90aade4-c554-4846-8b68-dcd7d8f8c461",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/4.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86781640-7003-4529-adae-a33424d40ca6",
   "metadata": {},
   "source": [
    "- 第二个训练样本的令牌 ID 为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3ddf6-9677-407b-9203-42e4eb40a37c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/embeddings-and-linear-layers/5.png\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cc5d70-c35b-4334-8db4-d5ab6bfe465c",
   "metadata": {},
   "source": [
    "- 由于每个独热编码行中除一个索引外的所有索引均为 0（这是设计使然），因此这种矩阵乘法本质上等同于查找独热元素。\n",
    "- 这种对独热编码进行矩阵乘法的方法等价于嵌入层查找，但如果处理大型嵌入矩阵，效率可能会很低，因为存在大量浪费的零乘法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
