{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e475425-8300-43f2-a5e8-6b5d2de59925",
   "metadata": {},
   "source": [
    "<!-- # Byte Pair Encoding (BPE) Tokenizer From Scratch -->\n",
    "# 从头开始的字节对编码（BPE）标记器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc3f3-8ec1-4fd3-b378-d9a3d7807a54",
   "metadata": {},
   "source": [
    "<!-- - This is a standalone notebook implementing the popular byte pair encoding (BPE) tokenization algorithm, which is used in models like GPT-2 to GPT-4, Llama 3, etc., from scratch for educational purposes -->\n",
    "- 这是一个独立的笔记本，实现了流行的字节对编码（BPE）标记化算法，该算法用于GPT-2到GPT-4， Llama 3等模型，从头开始用于教育目的\n",
    "<!-- - For more details about the purpose of tokenization, please refer to [Chapter 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb); this code here is bonus material explaining the BPE algorithm -->\n",
    "- 有关令牌化目的的更多详细信息，请参阅[第2章](../01_main-chapter-code/ch02.ipynb)；这里的代码是解释BPE算法的额外材料\n",
    "<!-- - The original BPE tokenizer that OpenAI implemented for training the original GPT models can be found [here](https://github.com/openai/gpt-2/blob/master/src/encoder.py) -->\n",
    "- OpenAI为训练原始GPT模型而实现的原始BPE标记器可以在[这里](https://github.com/openai/gpt-2/blob/master/src/encoder.py)找到。\n",
    "<!-- - The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage -->\n",
    "- BPE算法最初描述于1994年：Philip Gage的“[一种新的数据压缩算法](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)”\n",
    "<!-- - Most projects, including Llama 3, nowadays use OpenAI's open-source [tiktoken library](https://github.com/openai/tiktoken) due to its computational performance; it allows loading pretrained GPT-2 and GPT-4 tokenizers, for example (the Llama 3 models were trained using the GPT-4 tokenizer as well) -->\n",
    "- 由于其计算性能，包括Llama 3在内的大多数项目现在都使用OpenAI的开源[tiktoken库](https://github.com/openai/tiktoken)；例如，它允许加载预训练的GPT-2和GPT-4标记器（Llama 3模型也是使用GPT-4标记器进行训练的）\n",
    "<!-- - The difference between the implementations above and my implementation in this notebook, besides it being is that it also includes a function for training the tokenizer (for educational purposes) -->\n",
    "- 上面的实现和我在本笔记本中的实现之间的区别，除了它是，它还包括一个用于训练标记器的函数（出于学习目的）。\n",
    "<!-- - There's also an implementation called [minBPE](https://github.com/karpathy/minbpe) with training support, which is maybe more performant (my implementation here is focused on educational purposes); in contrast to `minbpe` my implementation additionally allows loading the original OpenAI tokenizer vocabulary and BPE \"merges\" (additionally, Hugging Face tokenizers are also capable of training and loading various tokenizers; see [this GitHub discussion](https://github.com/rasbt/LLMs-from-scratch/discussions/485) by a reader who trained a BPE tokenizer on the Nepali language for more info) -->\n",
    "- 还有一个名为[minBPE]（https://github.com/karpathy/minbpe）的实现，它具有培训支持，可能性能更高（我在这里的实现侧重于教育目的）；与“minbpe”相反，我的实现还允许加载原始的OpenAI标记器词汇表和BPE“合并”（此外，hug Face标记器也能够训练和加载各种标记器；参见[这个GitHub讨论](https://github.com/rasbt/LLMs-from-scratch/discussions/485)，由一位在尼泊尔语上训练BPE标记器的读者提供更多信息）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62336db-f45c-4894-9167-7583095dbdf1",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- # 1. The main idea behind byte pair encoding (BPE) -->\n",
    "# 1. 字节对编码（BPE）背后的主要思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f1231-bd42-41b5-a017-974b8c660a44",
   "metadata": {},
   "source": [
    "<!-- - The main idea in BPE is to convert text into an integer representation (token IDs) for LLM training (see [Chapter 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)) -->\n",
    "- BPE的主要思想是将文本转换为LLM训练的整数表示（token IDs）（参见[第2章](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb)）\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/bpe-overview.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c625d-26a1-4896-98a2-0fdcd1591256",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- ## 1.1 Bits and bytes -->\n",
    "## 1.1 字节对编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddaa35-0ed7-4012-827e-911de11c266c",
   "metadata": {},
   "source": [
    "<!-- - Before getting to the BPE algorithm, let's introduce the notion of bytes -->\n",
    "- 在讨论BPE算法之前，让我们先介绍一下字节的概念\n",
    "<!-- - Consider converting text into a byte array (BPE stands for \"byte\" pair encoding after all): -->\n",
    "- 考虑将文本转换为字节数组（毕竟BPE代表“字节”对编码）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9bc9e4-120f-4bac-8fa6-6523c568d12e",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd92a2a-9d74-4dc7-bb53-ac33d6cf2fab",
   "metadata": {},
   "source": [
    "<!-- - When we call `list()` on a `bytearray` object, each byte is treated as an individual element, and the result is a list of integers corresponding to the byte values: -->\n",
    "- 当我们在`bytearray`对象上调用`list()`时，每个字节都被视为一个单独的元素，结果是一个与字节值对应的整数列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c586945-d459-4f9a-855d-bf73438ef0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efea37-f4c3-4cb8-bfa5-9299175faf9a",
   "metadata": {},
   "source": [
    "<!-- - This would be a valid way to convert text into a token ID representation that we need for the embedding layer of an LLM -->\n",
    "- 这将是一种有效的方式，将文本转换为令牌ID表示，我们需要在LLM的嵌入层\n",
    "<!-- - However, the downside of this approach is that it is creating one ID for each character (that's a lot of IDs for a short text!) -->\n",
    "- 然而，这种方法的缺点是它为每个字符创建一个ID（对于一个短文本来说，这是很多ID !）\n",
    "<!-- - I.e., this means for a 17-character input text, we have to use 17 token IDs as input to the LLM: -->\n",
    "- 也就是说，这意味着对于17个字符的输入文本，我们必须使用17个令牌id作为LLM的输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d5b61d9-79a0-48b4-9b3e-64ab595c5b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc833a-c0d4-4d46-9180-c0042fd6addc",
   "metadata": {},
   "source": [
    "<!-- - If you have worked with LLMs before, you may know that the BPE tokenizers have a vocabulary where we have a token ID for whole words or subwords instead of each character -->\n",
    "- 如果您以前使用过llm，您可能知道BPE标记器有一个词汇表，其中我们为整个单词或子单词而不是每个字符提供令牌ID\n",
    "<!-- - For example, the GPT-2 tokenizer tokenizes the same text (\"This is some text\") into only 4 instead of 17 tokens: `1212, 318, 617, 2420` -->\n",
    "- 例如，GPT-2标记器将相同的文本（\"This is some text\"）标记为4个而不是17个标记： `1212, 318, 617, 2420`\n",
    "<!-- - You can double-check this using the interactive [tiktoken app](https://tiktokenizer.vercel.app/?model=gpt2) or the [tiktoken library](https://github.com/openai/tiktoken): -->\n",
    "- 你可以使用互动的[tiktoken应用程序](https://tiktokenizer.vercel.app/?model=gpt2)或[tiktoken库](https://github.com/openai/tiktoken):\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"800px\">\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b99de-cbfc-441c-8b3e-296a5dd7bb27",
   "metadata": {},
   "source": [
    "<!-- - Since a byte consists of 8 bits, there are 2<sup>8</sup> = 256 possible values that a single byte can represent, ranging from 0 to 255 -->\n",
    "- 由于一个字节由8位组成，因此有2<sup>8</sup> = 256个可能的值，单个字节可以表示，范围从0到255\n",
    "<!-- - You can confirm this by executing the code `bytearray(range(0, 257))`, which will warn you that `ValueError: byte must be in range(0, 256)`) -->\n",
    "- 你可以通过执行代码`bytearray(range(0, 257))`来确认这一点，它会警告你 `ValueError: byte must be in range(0, 256)`\n",
    "<!-- - A BPE tokenizer usually uses these 256 values as its first 256 single-character tokens; one could visually check this by running the following code: -->\n",
    "- BPE标记器通常使用这256个值作为其前256个单字符标记；可以通过运行以下代码来直观地检查：\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")\n",
    "\"\"\"\n",
    "prints:\n",
    "0: !\n",
    "1: \"\n",
    "2: #\n",
    "...\n",
    "255: �  # <---- single character tokens up to here\n",
    "256:  t\n",
    "257:  a\n",
    "...\n",
    "298: ent\n",
    "299:  n\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff0207-7f8e-44fa-9381-2a4bd83daab3",
   "metadata": {},
   "source": [
    "<!-- - Above, note that entries 256 and 257 are not single-character values but double-character values (a whitespace + a letter), which is a little shortcoming of the original GPT-2 BPE Tokenizer (this has been improved in the GPT-4 tokenizer) -->\n",
    "- 上面，请注意条目256和257不是单字符值，而是双字符值（一个空格+一个字母），这是原始GPT-2 BPE标记器的一个小缺点（这在GPT-4标记器中得到了改进）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241c23a-d487-488d-bded-cdf054e24920",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- ## 1.2 Building the vocabulary -->\n",
    "## 1.2 建立词汇表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2ceb7-0b3f-4a62-8dcc-07810cd8886e",
   "metadata": {},
   "source": [
    "<!-- - The goal of the BPE tokenization algorithm is to build a vocabulary of commonly occurring subwords like `298: ent` (which can be found in *entangle, entertain, enter, entrance, entity, ...*, for example), or even complete words like -->\n",
    "- BPE标记化算法的目标是建立一个常见子词的词汇表，如 `298: ent` （可以在*entangle， entertainment, enter, entrance, entity，…中找到）。*，例如)，甚至完整的单词，如\n",
    "\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d4420-a4c7-4813-916a-06f4f46bc3f0",
   "metadata": {},
   "source": [
    "<!-- - The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage -->\n",
    "- BPE算法最初描述于1994年：Philip Gage的\"[一种新的数据压缩算法](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\"\n",
    "<!-- - Before we get to the actual code implementation, the form that is used for LLM tokenizers today can be summarized as described in the following sections. -->\n",
    "- 在我们开始实际的代码实现之前，现在LLM标记器使用的形式可以总结为下面几节所描述的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc71db9-b070-48c4-8412-81f45b308ab3",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- ## 1.3 BPE algorithm outline -->\n",
    "## 1.3 BPE算法概要\n",
    "\n",
    "<!-- **1. Identify frequent pairs** -->\n",
    "**1. 识别频繁配对**\n",
    "<!-- - In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters) -->\n",
    "- 在每次迭代中，扫描文本以找到最常出现的字节对（或字符）\n",
    "\n",
    "<!-- **2. Replace and record** -->\n",
    "**2. 更换记录**\n",
    "\n",
    "<!-- - Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256) -->\n",
    "- 用一个新的占位符ID(一个尚未使用的，例如，如果我们从0开始…第一个占位符应该是256)\n",
    "<!-- - Record this mapping in a lookup table -->\n",
    "- 将此映射记录在查找表中\n",
    "<!-- - The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's 50,257) -->\n",
    "- 查找表的大小是一个超参数，也称为“词汇表大小”(对于GPT-2，它是 50,257)\n",
    "  \n",
    "\n",
    "<!-- **3. Repeat until no gains** -->\n",
    "**3.重复，直到没有收获**\n",
    "\n",
    "<!-- - Keep repeating steps 1 and 2, continually merging the most frequent pairs -->\n",
    "- 重复步骤1和2，不断合并最频繁的配对\n",
    "<!-- - Stop when no further compression is possible (e.g., no pair occurs more than once) -->\n",
    "- 当无法进一步压缩时停止（例如，没有一对出现超过一次）\n",
    "\n",
    "<!-- **Decompression (decoding)** -->\n",
    "**解压 (解码)**\n",
    "<!-- - To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table -->\n",
    "- 要恢复原始文本，可以使用查找表将每个ID替换为对应的对，从而逆转该过程\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ac9a-3528-4186-9468-8420c7b2ac00",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- ## 1.4 BPE algorithm example -->\n",
    "## 1.4 BPE算法示例\n",
    "\n",
    "<!-- ### 1.4.1 Concrete example of the encoding part (steps 1 & 2 in section 1.3) -->\n",
    "### 1.4.1编码部分的具体示例（1.3节第1、2步）\n",
    "\n",
    "<!-- - Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE tokenizer -->\n",
    "- 假设我们有一个文本（训练数据集）“帽子里的猫”，我们想要从中构建BPE标记器的词汇表\n",
    "\n",
    "<!-- **Iteration 1** -->\n",
    "**迭代1**\n",
    "\n",
    "<!-- 1. Identify frequent pairs -->\n",
    "1. 识别频繁对\n",
    "  <!-- - In this text, \"th\" appears twice (at the beginning and before the second \"e\") -->\n",
    "  - 在这篇文章中，“th”出现了两次（在开头和第二个“e”之前）\n",
    "\n",
    "<!-- 2. Replace and record -->\n",
    "2. 更换并记录\n",
    "  <!-- - replace \"th\" with a new token ID that is not already in use, e.g., 256 -->\n",
    "  - 将“th”替换为尚未使用的新令牌ID，例如256\n",
    "  <!-- - the new text is: `<256>e cat in <256>e hat` -->\n",
    "  - 新文本是： `<256>e cat in <256>e hat`\n",
    "  <!-- - the new vocabulary is -->\n",
    "  - 新词汇是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "<!-- **Iteration 2** -->\n",
    "**迭代 2**\n",
    "\n",
    "<!-- 1. **Identify frequent pairs** -->\n",
    "1. **识别频繁对**\n",
    "   <!-- - In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears twice -->\n",
    "   - 在文本 `<256>e cat in <256>e hat`中，对 `<256>e`出现两次\n",
    "\n",
    "<!-- 2. **Replace and record** -->\n",
    "2.  **更换并记录**\n",
    "   <!-- - replace `<256>e` with a new token ID that is not already in use, for example, `257`. -->\n",
    "   - 将 `<256>e` 替换为一个尚未使用的新令牌ID，例如`257`。\n",
    "   <!-- - The new text is: -->\n",
    "   - 新文本是：\n",
    "     ```\n",
    "     <257> cat in <257> hat\n",
    "     ```\n",
    "   <!-- - The updated vocabulary is: -->\n",
    "   - 更新后的词汇是：\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     ```\n",
    "\n",
    "<!-- **Iteration 3** -->\n",
    "**迭代 3**\n",
    "<!-- 1. **Identify frequent pairs**   -->\n",
    "1. **识别频繁对**\n",
    "   <!-- - In the text `<257> cat in <257> hat`, the pair `<257> ` appears twice (once at the beginning and once before “hat”). -->\n",
    "   - 在文本 `<257> cat in <257> hat`中，对 `<257> `出现了两次（一次在开头，一次在“hat”之前）。\n",
    "\n",
    "<!-- 2. **Replace and record**   -->\n",
    "2.  **更换并记录**\n",
    "   <!-- - replace `<257> ` with a new token ID that is not already in use, for example, `258`. -->\n",
    "   - 将`<257> `替换为尚未使用的新令牌ID，例如`258`。\n",
    "   <!-- - the new text is: -->\n",
    "   - 新文本为：\n",
    "     ```\n",
    "     <258>cat in <258>hat\n",
    "     ```\n",
    "   <!-- - The updated vocabulary is: -->\n",
    "   - 更新后的词汇为：\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     258: \"<257> \"\n",
    "     ```\n",
    "     \n",
    "<!-- - and so forth -->\n",
    "- 等等\n",
    "\n",
    "&nbsp;\n",
    "<!-- ### 1.4.2 Concrete example of the decoding part (step 3 in section 1.3) -->\n",
    "### 1.4.2 解码部分的具体示例（1.3节中的步骤3）\n",
    "<!-- - To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced -->\n",
    "- 为了恢复原始文本，我们通过将每个令牌 ID 替换为其对应的对（按照它们引入的相反顺序）来反转该过程\n",
    "<!-- - Start with the final compressed text: `<258>cat in <258>hat` -->\n",
    "- 从最终的压缩文本开始 `<258>cat in <258>hat`\n",
    "<!-- -  Substitute `<258>` → `<257> `: `<257> cat in <257> hat` -->\n",
    "-  替换 `<258>` → `<257> `: `<257> cat in <257> hat`\n",
    "<!-- - Substitute `<257>` → `<256>e`: `<256>e cat in <256>e hat` -->\n",
    "- 替换 `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "<!-- - Substitute `<256>` → \"th\": `the cat in the hat` -->\n",
    "- 替换 `<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2324948-ddd0-45d1-8ba8-e8eda9fc6677",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- ## 2. A simple BPE implementation -->\n",
    "## 2. 一个简单的 BPE 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca709-40d7-4e3d-bf3e-4f5687a2e19b",
   "metadata": {},
   "source": [
    "<!-- - Below is an implementation of this algorithm described above as a Python class that mimics the `tiktoken` Python user interface -->\n",
    "- 下面是上述算法的实现，作为一个 Python 类，模仿`tiktoken` Python 用户界面\n",
    "<!-- - Note that the encoding part above describes the original training step via `train()`; however, the `encode()` method works similarly (although it looks a bit more complicated because of the special token handling): -->\n",
    "- 请注意，上面的编码部分通过 `train()`描述了原始训练步骤；然而，`encode()` 方法的工作原理类似（尽管由于特殊的令牌处理，它看起来有点复杂）：\n",
    "\n",
    "<!-- 1. Split the input text into individual bytes -->\n",
    "1. 将输入文本拆分为单独的字节\n",
    "<!-- 2. Repeatedly find & replace (merge) adjacent tokens (pairs) when they match any pair in the learned BPE merges (from highest to lowest \"rank,\" i.e., in the order they were learned) -->\n",
    "2. 当相邻标记（对）与学习的 BPE 合并中的任何对匹配时，重复查找和替换（合并）相邻标记（对）（从最高到最低“排名”，即按照学习的顺序）\n",
    "<!-- 3. Continue merging until no more merges can be applied -->\n",
    "3. 继续合并，直到无法再应用合并为止\n",
    "<!-- 4. The final list of token IDs is the encoded output -->\n",
    "4. 最终的令牌 ID 列表是编码输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e4a15ec-2667-4f56-b7c1-34e8071b621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
    "        #  of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with \"Ġ\"\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # encoder.json is {token_str: id}; we want id->str and str->id\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "    \n",
    "        # Must have GPT-2's printable newline character 'Ċ' (U+010A) at id 198\n",
    "        if \"Ċ\" not in self.inverse_vocab or self.inverse_vocab[\"Ċ\"] != 198:\n",
    "            raise KeyError(\"Vocabulary missing GPT-2 newline glyph 'Ċ' at id 198.\")\n",
    "    \n",
    "        # Must have <|endoftext|> at 50256\n",
    "        if \"<|endoftext|>\" not in self.inverse_vocab or self.inverse_vocab[\"<|endoftext|>\"] != 50256:\n",
    "            raise KeyError(\"Vocabulary missing <|endoftext|> at id 50256.\")\n",
    "    \n",
    "        # Provide a convenience alias for '\\n' -> 198\n",
    "        # Keep printable character 'Ċ' in vocab so BPE merges keep working\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            self.inverse_vocab[\"\\n\"] = self.inverse_vocab[\"Ċ\"]\n",
    "\n",
    "        if \"\\r\" not in self.inverse_vocab:\n",
    "            if 201 in self.vocab:\n",
    "                self.inverse_vocab[\"\\r\"] = 201\n",
    "            else:\n",
    "                raise KeyError(\"Vocabulary missing carriage return token at id 201.\")\n",
    "\n",
    "        # Load GPT-2 merges and store ranks\n",
    "        self.bpe_ranks = {}\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "    \n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                token1, *rest = line.strip().split()\n",
    "                if len(rest) != 1:\n",
    "                    continue\n",
    "                token2 = rest[0]\n",
    "                if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                    self.bpe_ranks[(token1, token2)] = rank\n",
    "                    rank += 1\n",
    "                else:\n",
    "                    # Safe to skip pairs whose symbols are not in vocab\n",
    "                    pass\n",
    "\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "    \n",
    "        # ---- This section is to mimic tiktoken in terms of allowed special tokens ----\n",
    "        specials_in_vocab = [\n",
    "            tok for tok in self.inverse_vocab\n",
    "            if tok.startswith(\"<|\") and tok.endswith(\"|>\")\n",
    "        ]\n",
    "        if allowed_special is None:\n",
    "            # Nothing is allowed\n",
    "            disallowed = [tok for tok in specials_in_vocab if tok in text]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "        else:\n",
    "            # Some spefic tokens are allowed (e.g., we use this for <|endoftext|>)\n",
    "            disallowed = [tok for tok in specials_in_vocab if tok in text and tok not in allowed_special]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "        # -----------------------------------------------------------------------------\n",
    "\n",
    "        token_ids = []\n",
    "        # If some specials are allowed, split around them and passthrough those ids\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            special_pattern = \"(\" + \"|\".join(\n",
    "                re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)\n",
    "            ) + \")\"\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # encode prefix normally\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # remainder to process normally\n",
    "    \n",
    "            # Extra guard for any other special literals left over\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "\n",
    "    \n",
    "        # ---- Newline and carriage return handling ----\n",
    "        tokens = []\n",
    "        parts = re.split(r'(\\r\\n|\\r|\\n)', text)\n",
    "        for part in parts:\n",
    "            if part == \"\":\n",
    "                continue\n",
    "            if part == \"\\r\\n\":\n",
    "                tokens.append(\"\\r\")\n",
    "                tokens.append(\"\\n\")\n",
    "                continue\n",
    "            if part == \"\\r\":\n",
    "                tokens.append(\"\\r\")\n",
    "                continue\n",
    "            if part == \"\\n\":\n",
    "                tokens.append(\"\\n\")\n",
    "                continue\n",
    "    \n",
    "            # Normal chunk without line breaks:\n",
    "            # - If spaces precede a word, prefix the first word with 'Ġ' and\n",
    "            #   add standalone 'Ġ' for additional spaces\n",
    "            # - If spaces trail the chunk (e.g., before a newline) add\n",
    "            #   standalone 'Ġ' tokens (tiktoken produces id 220 for 'Ġ')\n",
    "            pending_spaces = 0\n",
    "            for m in re.finditer(r'( +)|(\\S+)', part):\n",
    "                if m.group(1) is not None:\n",
    "                    pending_spaces += len(m.group(1))\n",
    "                else:\n",
    "                    word = m.group(2)\n",
    "                    if pending_spaces > 0:\n",
    "                        tokens.append(\"Ġ\" + word) # one leading space\n",
    "                        for _ in range(pending_spaces - 1):\n",
    "                            tokens.append(\"Ġ\")  # remaining spaces as standalone\n",
    "                        pending_spaces = 0\n",
    "                    else:\n",
    "                        tokens.append(word)\n",
    "            # Trailing spaces (no following word): add standalone 'Ġ' tokens\n",
    "            for _ in range(pending_spaces):\n",
    "                tokens.append(\"Ġ\")\n",
    "        # ---------------------------------------------------------------\n",
    "    \n",
    "        # Map tokens -> ids (BPE if needed)\n",
    "        for tok in tokens:\n",
    "            if tok in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[tok])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(tok))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        # If we haven't loaded OpenAI's GPT-2 merges, use my approach\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        # Uncomment for educational purposes:\n",
    "                        # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                        i += 2  # Skip the next token as it's merged\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "\n",
    "        # Otherwise, do GPT-2-style merging with the ranks:\n",
    "        # 1) Convert token_ids back to string \"symbols\" for each ID\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "\n",
    "        # Repeatedly merge all occurrences of the lowest-rank pair\n",
    "        while True:\n",
    "            # Collect all adjacent pairs\n",
    "            pairs = set(zip(symbols, symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Find the pair with the best (lowest) rank\n",
    "            min_rank = float(\"inf\")\n",
    "            bigram = None\n",
    "            for p in pairs:\n",
    "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
    "                if r < min_rank:\n",
    "                    min_rank = r\n",
    "                    bigram = p\n",
    "\n",
    "            # If no valid ranked pair is present, we're done\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            # Merge all occurrences of that pair\n",
    "            first, second = bigram\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # If we see (first, second) at position i, merge them\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                    new_symbols.append(first + second)  # merged symbol\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "\n",
    "        # Finally, convert merged symbols back to IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for tid in token_ids:\n",
    "            if tid not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {tid} not found in vocab.\")\n",
    "            tok = self.vocab[tid]\n",
    "\n",
    "            # Map GPT-2 special chars back to real chars\n",
    "            if tid == 198 or tok == \"\\n\":\n",
    "                out.append(\"\\n\")\n",
    "            elif tid == 201 or tok == \"\\r\":\n",
    "                out.append(\"\\r\")\n",
    "            elif tok.startswith(\"Ġ\"):\n",
    "                out.append(\" \" + tok[1:])\n",
    "            else:\n",
    "                out.append(tok)\n",
    "        return \"\".join(out)\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db7310-79c7-4ee0-b5fa-d760c6e1aa67",
   "metadata": {},
   "source": [
    "<!-- - There is a lot of code in the `BPETokenizerSimple` class above, and discussing it in detail is out of scope for this notebook, but the next section offers a short overview of the usage to understand the class methods a bit better -->\n",
    "- 上面的“BPETokenizerSimple”类中有很多代码，详细讨论它超出了本笔记本的范围，但下一节提供了用法的简短概述，以便更好地理解类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe1836-eed4-40dc-860b-2d23074d067e",
   "metadata": {},
   "source": [
    "<!-- ## 3. BPE implementation walkthrough -->\n",
    "## 3. BPE 实施演练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c996c-fd34-484f-a877-13d977214cf7",
   "metadata": {},
   "source": [
    "<!-- - In practice, I highly recommend using [tiktoken](https://github.com/openai/tiktoken) as my implementation above focuses on readability and educational purposes, not on performance -->\n",
    "- 在实践中，我强烈建议使用 [tiktoken](https://github.com/openai/tiktoken)，因为我上面的实现侧重于可读性和学习目的，而不是性能\n",
    "<!-- - However, the usage is more or less similar to tiktoken, except that tiktoken does not have a training method -->\n",
    "- 不过用法或多或少和tiktoken类似，只不过tiktoken没有训练方法\n",
    "<!-- - Let's see how my `BPETokenizerSimple` Python code above works by looking at some examples below (a detailed code discussion is out of scope for this notebook) -->\n",
    "- 让我们通过查看下面的一些示例来了解上面的`BPETokenizerSimple`Python 代码是如何工作的（详细的代码讨论超出了本笔记本的范围）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82acaf6-7ed5-4d3b-81c0-ae4d3559d2c7",
   "metadata": {},
   "source": [
    "<!-- ### 3.1 Training, encoding, and decoding -->\n",
    "### 3.1 训练、编码和解码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bf037-903e-4555-b09c-206e1a410278",
   "metadata": {},
   "source": [
    "<!-- - First, let's consider some sample text as our training dataset: -->\n",
    "- 首先，让我们考虑一些示例文本作为我们的训练数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51872c08-e01b-40c3-a8a0-e8d6a773e3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the-verdict.txt already exists in ../01_main-chapter-code/the-verdict.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file_if_absent(url, filename, search_dirs):\n",
    "    for directory in search_dirs:\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"{filename} already exists in {file_path}\")\n",
    "            return file_path\n",
    "\n",
    "    target_path = os.path.join(search_dirs[0], filename)\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "        with open(target_path, \"wb\") as out_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    out_file.write(chunk)\n",
    "        print(f\"Downloaded {filename} to {target_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filename}. Error: {e}\")\n",
    "\n",
    "    return target_path\n",
    "\n",
    "\n",
    "verdict_path = download_file_if_absent(\n",
    "    url=(\n",
    "         \"https://raw.githubusercontent.com/rasbt/\"\n",
    "         \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "         \"the-verdict.txt\"\n",
    "    ),\n",
    "    filename=\"the-verdict.txt\",\n",
    "    search_dirs=[\"ch02/01_main-chapter-code/\", \"../01_main-chapter-code/\", \".\"]\n",
    ")\n",
    "\n",
    "with open(verdict_path, \"r\", encoding=\"utf-8\") as f: # added ../01_main-chapter-code/\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1b6ac-71d3-4817-956a-9bc7e463a84a",
   "metadata": {},
   "source": [
    "<!-- - Next, let's initialize and train the BPE tokenizer with a vocabulary size of 1,000 -->\n",
    "- 接下来，我们初始化并训练词汇量为 1,000 的 BPE 分词器\n",
    "<!-- - Note that the vocabulary size is already 256 by default due to the byte values discussed earlier, so we are only \"learning\" 744 vocabulary entries (if we consider the `<|endoftext|>` special token and the `Ġ` whitespace token; so, that's 742 to be precise) -->\n",
    "- 请注意，由于前面讨论的字节值，默认情况下词汇表大小已经为 256，因此我们仅“学习”744 个词汇表条目（如果我们考虑 `<|endoftext|>` 特殊标记和 `Ġ` 空白标记；因此，准确地说是 742 个）\n",
    "<!-- - For comparison, the GPT-2 vocabulary is 50,257 tokens, the GPT-4 vocabulary is 100,256 tokens (`cl100k_base` in tiktoken), and GPT-4o uses 199,997 tokens (`o200k_base` in tiktoken); they have all much bigger training sets compared to our simple example text above -->\n",
    "- 作为比较，GPT-2 词汇表为 50,257 个标记，GPT-4 词汇表为 100,256 个标记（tiktoken 中的`cl100k_base` ），GPT-4o 使用 199,997 个标记（tiktoken 中的`o200k_base`）；与我们上面的简单示例文本相比，它们都有更大的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027348fd-d52f-4396-93dd-38eed142df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474ff05-5629-4f13-9e03-a47b1e713850",
   "metadata": {},
   "source": [
    "<!-- - You may want to inspect the vocabulary contents (but note it will create a long list) -->\n",
    "- 您可能想检查词汇内容（但请注意，它将创建一个很长的列表）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f705a283-355e-4460-b940-06bbc2ae4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9da0f-8a18-41cd-91ea-9ccc2bb5febb",
   "metadata": {},
   "source": [
    "<!-- - This vocabulary is created by merging 742 times (`= 1000 - len(range(0, 256)) - len(special_tokens) - \"Ġ\" = 1000 - 256 - 1 - 1 = 742`) -->\n",
    "- 该词汇表是通过合并 742 次创建的 (`= 1000 - len(range(0, 256)) - len(special_tokens) - \"Ġ\" = 1000 - 256 - 1 - 1 = 742`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da42d1c-f75c-4ba7-a6c5-4cb8543d4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac69c9-8413-482a-8148-6b2afbf1fb89",
   "metadata": {},
   "source": [
    "<!-- - This means that the first 256 entries are single-character tokens -->\n",
    "- 这意味着前 256 个条目是单字符标记"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a4108-7c8b-4b98-9c67-d622e9cdf250",
   "metadata": {},
   "source": [
    "<!-- - Next, let's use the created merges via the `encode` method to encode some text: -->\n",
    "- 接下来，让我们通过`encode` 方法使用创建的合并来编码一些文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1db5cce-e015-412b-ad56-060b8b638078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0331d37d-49a3-44f7-9aa9-9834e0938741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46, 257, 256]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.<|endoftext|> \"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<|endoftext|>\"})\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ed1b344-f7d4-4e9e-ac34-2a04b5c5b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 56\n",
      "Number of token IDs: 22\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1cfb9-402a-4e1e-9678-0b7547406248",
   "metadata": {},
   "source": [
    "<!-- - From the lengths above, we can see that a 42-character sentence was encoded into 20 token IDs, effectively cutting the input length roughly in half compared to a character-byte-based encoding -->\n",
    "- 从上面的长度中，我们可以看到一个 42 个字符的句子被编码为 20 个 token ID，与基于字符字节的编码相比，有效地将输入长度大约减少了一半"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252693ee-e806-4dac-ab76-2c69086360f4",
   "metadata": {},
   "source": [
    "<!-- - Note that the vocabulary itself is used in the `decode()` method, which allows us to map the token IDs back into text: -->\n",
    "- 请注意，词汇表本身在`decode()`方法中使用，它允许我们将标记 ID 映射回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da0e1faf-1933-43d9-b681-916c282a8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46, 257, 256]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b690e83-5d6b-409a-804e-321c287c24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.<|endoftext|> \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea5d09-e5ef-4721-994b-b9b25662fa0a",
   "metadata": {},
   "source": [
    "<!-- - Iterating over each token ID can give us a better understanding of how the token IDs are decoded via the vocabulary: -->\n",
    "- 迭代每个令牌 ID 可以让我们更好地理解令牌 ID 如何通过词汇表进行解码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b9e6289-92cb-4d88-b3c8-e836d7c8095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "287 ->  a\n",
      "466 -> nd\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n",
      "257 -> <|endoftext|>\n",
      "256 ->  \n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea41c6c-5538-4fd5-8b5f-195960853b71",
   "metadata": {},
   "source": [
    "<!-- - As we can see, most token IDs represent 2-character subwords; that's because the training data text is very short with not that many repetitive words, and because we used a relatively small vocabulary size -->\n",
    "- 正如我们所看到的，大多数令牌 ID 代表 2 个字符的子词；这是因为训练数据文本非常短，没有那么多重复单词，而且我们使用的词汇量相对较小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600055a3-7ec8-4abf-b88a-c4186fb71463",
   "metadata": {},
   "source": [
    "<!-- - As a summary, calling `decode(encode())` should be able to reproduce arbitrary input texts: -->\n",
    "- 总而言之，调用`decode(encode())` 应该能够重现任意输入文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7056cb1-a9a3-4cf6-8364-29fb493ae240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37bc6753-8f35-4ec7-b23e-df4a12103cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b42bb-55bc-4c9d-b859-457a28b76302",
   "metadata": {},
   "source": [
    "<!-- ### 3.2 Saving and loading the tokenizer -->\n",
    "### 3.2 保存和加载分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86210925-06dc-4e8c-87bd-821569cd7142",
   "metadata": {},
   "source": [
    "<!-- - Next, let's look at how we can save the trained tokenizer for reuse later: -->\n",
    "- 接下来，让我们看看如何保存经过训练的分词器以供以后重用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "955181cb-0910-4c6a-9c22-d8292a3ec1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained tokenizer\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e5ccfe7-ac67-42f3-b727-87886a8867f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"vocab.json\", bpe_merges_path=\"bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f9bcc2-3b27-4473-b75e-4f289d52a7cc",
   "metadata": {},
   "source": [
    "<!-- - The loaded tokenizer should be able to produce the same results as before: -->\n",
    "- 加载的分词器应该能够产生与以前相同的结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00d9bf8f-756f-48bf-81b8-b890e2c2ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.<|endoftext|> \n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7addb64-2892-4e1c-85dd-4f5152740099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(\n",
    "    tokenizer2.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d10b2-1ab8-44ee-b51a-14248e30d662",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- ### 3.3 Loading the original GPT-2 BPE tokenizer from OpenAI -->\n",
    "### 3.3 从 OpenAI 加载原始 GPT-2 BPE 标记器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df07e031-9495-4af1-929f-3f16cbde82a5",
   "metadata": {},
   "source": [
    "<!-- - Finally, let's load OpenAI's GPT-2 tokenizer files -->\n",
    "- 最后，让我们加载 OpenAI 的 GPT-2 分词器文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b45b4366-2c2b-4309-9a14-febf3add8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab.bpe already exists in ../02_bonus_bytepair-encoder/gpt2_model/vocab.bpe\n",
      "encoder.json already exists in ../02_bonus_bytepair-encoder/gpt2_model/encoder.json\n"
     ]
    }
   ],
   "source": [
    "# Download files if not already present in this directory\n",
    "\n",
    "# Define the directories to search and the files to download\n",
    "search_directories = [\"ch02/02_bonus_bytepair-encoder/gpt2_model/\", \"../02_bonus_bytepair-encoder/gpt2_model/\", \".\"]\n",
    "\n",
    "files_to_download = {\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe\": \"vocab.bpe\",\n",
    "    \"https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json\": \"encoder.json\"\n",
    "}\n",
    "\n",
    "# Ensure directories exist and download files if needed\n",
    "paths = {}\n",
    "for url, filename in files_to_download.items():\n",
    "    paths[filename] = download_file_if_absent(url, filename, search_directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe260a0-1d5f-4bbd-9934-5117052764d1",
   "metadata": {},
   "source": [
    "<!-- - Next, we load the files via the `load_vocab_and_merges_from_openai` method: -->\n",
    "- 接下来，我们通过`load_vocab_and_merges_from_openai`方法加载文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74306e6c-47d3-45a3-9e0f-93f7303ef601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = BPETokenizerSimple()\n",
    "tokenizer_gpt2.load_vocab_and_merges_from_openai(\n",
    "    vocab_path=paths[\"encoder.json\"], bpe_merges_path=paths[\"vocab.bpe\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d012ce-9e87-47d7-8a1b-b6d6294d76c0",
   "metadata": {},
   "source": [
    "<!-- - The vocabulary size should be `50257` as we can confirm via the code below: -->\n",
    "- 词汇量大小应为“50257”，我们可以通过以下代码确认："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bb722b4-dbf5-4a0c-9120-efda3293f132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_gpt2.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea44b45-f524-44b5-a53a-f6d7f483fc19",
   "metadata": {},
   "source": [
    "<!-- - We can now use the GPT-2 tokenizer via our `BPETokenizerSimple` object: -->\n",
    "- 现在，我们可以通过 `BPETokenizerSimple`对象使用 GPT-2 分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4866de7-fb32-4dd6-a878-469ec734641c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1212, 318, 617, 2420]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"This is some text\"\n",
    "token_ids = tokenizer_gpt2.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3da8d9b2-af55-4b09-95d7-fabd983e919e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is some text\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_gpt2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1e2dc-f69b-4533-87ef-549e6fb9b5a0",
   "metadata": {},
   "source": [
    "<!-- - You can double-check that this produces the correct tokens using the interactive [tiktoken app](https://tiktokenizer.vercel.app/?model=gpt2) or the [tiktoken library](https://github.com/openai/tiktoken): -->\n",
    "- 您可以使用交互式 [tiktoken 应用程序](https://tiktokenizer.vercel.app/?model=gpt2) 或 [tiktoken 库](https://github.com/openai/tiktoken),仔细检查这会产生正确的令牌：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"800px\">\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558af04-483c-4f6b-88f5-a534f37316cd",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "<!-- # 4. Conclusion -->\n",
    "# 4. 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ed0e6-ad06-4bb3-bb39-6b8110c1caa4",
   "metadata": {},
   "source": [
    "<!-- - That's it! That's how BPE works in a nutshell, complete with a training method for creating new tokenizers or loading the GPT-2 tokenizer vocabular and merges from the original OpenAI GPT-2 model -->\n",
    "- 就是这样！简而言之，这就是 BPE 的工作原理，包括用于创建新分词器或加载 GPT-2 分词器词汇并与原始 OpenAI GPT-2 模型合并的训练方法\n",
    "<!-- - I hope you found this brief tutorial useful for educational purposes; if you have any questions, please feel free to open a new Discussion [here](https://github.com/rasbt/LLMs-from-scratch/discussions/categories/q-a) -->\n",
    "- 我希望您发现这个简短的教程对教育目的有用；如果您有任何疑问，请随时在[此处](https://github.com/rasbt/LLMs-from-scratch/discussions/categories/q-a)打开新的讨论\n",
    "<!-- - For a performance comparison with other tokenizer implementations, please see [this notebook](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb) -->\n",
    "- 有关与其他分词器实现的性能比较，请参阅[本笔记本](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
