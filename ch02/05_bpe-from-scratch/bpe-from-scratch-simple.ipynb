{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e475425-8300-43f2-a5e8-6b5d2de59925",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding (BPE) Tokenizer From Scratch -- Simple 从零构建BPE分词器 -- 简易版"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bfc3f3-8ec1-4fd3-b378-d9a3d7807a54",
   "metadata": {},
   "source": [
    " <!--  -This is a standalone notebook implementing the popular byte pair encoding (BPE) tokenization algorithm, which is used in models like GPT-2 to GPT-4, Llama 3, etc., from scratch for educational purposes -->\n",
    "- 这是一个独立的笔记本，从零开始实现了流行的字节对编码 (BPE) 分词算法，该算法被应用于 GPT-2 到 GPT-4、Llama 3 等模型中，用于学习目的。\n",
    " <!--  - For more details about the purpose of tokenization, please refer to [Chapter 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb); this code here is bonus material explaining the BPE algorithm-->\n",
    "- 有关分词目的的更多详细信息，请参考[第二章 ch02.ipynb](../01_main-chapter-code/ch02.ipynb);这段代码是解释 BPE 算法的附加材料\n",
    " <!--  - The original BPE tokenizer that OpenAI implemented for training the original GPT models can be found [here](https://github.com/openai/gpt-2/blob/master/src/encoder.py)-->\n",
    "- OpenAI 为训练原始 GPT 模型而实现的原始 BPE 分词器可以在[这里](https://github.com/openai/gpt-2/blob/master/src/encoder.py)找到\n",
    " <!--  - The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage-->\n",
    "- BPE 算法最初由 Philip Gage 于 1994 年提出：[一种新的数据压缩算法](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\n",
    " <!--  - Most projects, including Llama 3, nowadays use OpenAI's open-source [tiktoken library](https://github.com/openai/tiktoken) due to its computational performance; it allows loading pretrained GPT-2 and GPT-4 tokenizers, for example (the Llama 3 models were trained using the GPT-4 tokenizer as well)-->\n",
    "- 由于其计算性能，现在大多数项目（包括 Llama 3）都使用 OpenAI 的开源[tiktoken 库](https://github.com/openai/tiktoken)，它允许加载预训练的 GPT-2 和 GPT-4 分词器，例如（Llama 3 模型也是使用 GPT-4 分词器训练的）。\n",
    " <!--  - The difference between the implementations above and my implementation in this notebook, besides it being is that it also includes a function for training the tokenizer (for educational purposes)-->\n",
    "- 上述实现与我在本笔记本中的实现的区别在于，我的实现还包含一个用于训练分词器的函数（用于学习目的）。\n",
    " <!--  - There's also an implementation called [minBPE](https://github.com/karpathy/minbpe) with training support, which is maybe more performant (my implementation here is focused on educational purposes); in contrast to `minbpe` my implementation additionally allows loading the original OpenAI tokenizer vocabulary and merges-->\n",
    "- 还有一个名为 [minBPE](https://github.com/karpathy/minbpe) 的实现，它支持训练，性能可能更高（我的实现侧重于教学目的）；与 `minbpe` 不同的是，我的实现还允许加载原始的 OpenAI 分词器词汇表并进行合并。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910acd61-8947-4cfa-962f-16f4c733f2db",
   "metadata": {},
   "source": [
    "<!-- **This is a very naive implementation for educational purposes. The [bpe-from-scratch.ipynb](bpe-from-scratch.ipynb) notebook contains a more sophisticated (but much harder to read) implementation that matches the behavior in tiktoken.**-->\n",
    "**这是一个非常简单的教学用途实现。[bpe-from-scratch.ipynb](bpe-from-scratch.ipynb) notebook 中包含一个更复杂（但更难读懂）的实现，它与 tiktoken 的行为相匹配。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62336db-f45c-4894-9167-7583095dbdf1",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 1. <!--The main idea behind byte pair encoding (BPE)--> 字节对编码（BPE）背后的主要思想"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f1231-bd42-41b5-a017-974b8c660a44",
   "metadata": {},
   "source": [
    "<!-- - The main idea in BPE is to convert text into an integer representation (token IDs) for LLM training (see [Chapter 2](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb))-->\n",
    "- BPE 的主要思想是将文本转换为整数表示（标记 ID），以便进行 LLM 训练.(详情参考[第二章 ch02.ipynb](../01_main-chapter-code/ch02.ipynb))\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/bpe-overview.webp\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c625d-26a1-4896-98a2-0fdcd1591256",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.1 Bits and bytes(比特与字节)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ddaa35-0ed7-4012-827e-911de11c266c",
   "metadata": {},
   "source": [
    "<!-- - Before getting to the BPE algorithm, let's introduce the notion of bytes-->\n",
    "- 在介绍 BPE 算法之前，我们先来了解一下字节的概念。\n",
    "<!-- - Consider converting text into a byte array (BPE stands for \"byte\" pair encoding after all):-->\n",
    "- 考虑将文本转换为字节数组（毕竟 BPE 代表的是“字节”对编码）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c9bc9e4-120f-4bac-8fa6-6523c568d12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'This is some text')\n"
     ]
    }
   ],
   "source": [
    "text = \"This is some text\"\n",
    "byte_ary = bytearray(text, \"utf-8\")\n",
    "print(byte_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd92a2a-9d74-4dc7-bb53-ac33d6cf2fab",
   "metadata": {},
   "source": [
    "<!-- - When we call `list()` on a `bytearray` object, each byte is treated as an individual element, and the result is a list of integers corresponding to the byte values:-->\n",
    "- 当我们对 `bytearray` 对象调用 `list()` 时，每个字节都被视为一个单独的元素，结果是一个与字节值对应的整数列表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c586945-d459-4f9a-855d-bf73438ef0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84, 104, 105, 115, 32, 105, 115, 32, 115, 111, 109, 101, 32, 116, 101, 120, 116]\n"
     ]
    }
   ],
   "source": [
    "ids = list(byte_ary)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efea37-f4c3-4cb8-bfa5-9299175faf9a",
   "metadata": {},
   "source": [
    "<!-- - This would be a valid way to convert text into a token ID representation that we need for the embedding layer of an LLM\n",
    "- However, the downside of this approach is that it is creating one ID for each character (that's a lot of IDs for a short text!)\n",
    "- I.e., this means for a 17-character input text, we have to use 17 token IDs as input to the LLM:-->\n",
    "- 这是一种将文本转换成LLM嵌入层所需的token ID表示有效方法。然而，这种方法的缺点是他为每个字符创建一个ID(对于短文本身来说，这将生成大量ID),\n",
    "- 也就是说，对应17个字符的输入文本，我们所需使用的17个token ID作为LLM的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5b61d9-79a0-48b4-9b3e-64ab595c5b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 17\n",
      "Number of token IDs: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(text))\n",
    "print(\"Number of token IDs:\", len(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc833a-c0d4-4d46-9180-c0042fd6addc",
   "metadata": {},
   "source": [
    "<!-- - If you have worked with LLMs before, you may know that the BPE tokenizers have a vocabulary where we have a token ID for whole words or subwords instead of each character\n",
    "- For example, the GPT-2 tokenizer tokenizes the same text (\"This is some text\") into only 4 instead of 17 tokens: `1212, 318, 617, 2420`\n",
    "- You can double-check this using the interactive [tiktoken app](https://tiktokenizer.vercel.app/?model=gpt2) or the [tiktoken library](https://github.com/openai/tiktoken):-->\n",
    "- 如果您之前使用过 LLM，您可能知道 BPE 分词器有一个词汇表，其中我们为整个单词或子词分配一个词元 ID，而不是为每个字符分配一个词元 ID。\n",
    "- 例如，GPT-2分词器将相同的文本(\"This is some text\") 分成只有4个次元而不是17个次元ID:`1212, 318, 617, 2420`\n",
    "- 你可以用交互[token软件](https://tiktokenizer.vercel.app/?model=gpt2)或者[token库](https://github.com/openai/tiktoken)进行二次验证：\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/bpe-from-scratch/tiktokenizer.webp\" width=\"800px\">\n",
    "\n",
    "```python\n",
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints [1212, 318, 617, 2420]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8787aa3-e920-4b29-ab7b-829152c96f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1212, 318, 617, 2420]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "gpt2_tokenizer.encode(\"This is some text\")\n",
    "# prints( [1212, 318, 617, 2420])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b99de-cbfc-441c-8b3e-296a5dd7bb27",
   "metadata": {},
   "source": [
    "<!-- - Since a byte consists of 8 bits, there are 2<sup>8</sup> = 256 possible values that a single byte can represent, ranging from 0 to 255\n",
    "- You can confirm this by executing the code `bytearray(range(0, 257))`, which will warn you that `ValueError: byte must be in range(0, 256)`)\n",
    "- A BPE tokenizer usually uses these 256 values as its first 256 single-character tokens; one could visually check this by running the following code:-->\n",
    "- 由于一个字节由 8 位组成，因此一个字节可以表示 2⁸ = 256 个可能的值，范围从 0 到 255。您可以通过执行代码 `bytearray(range(0, 257))` 来验证这一点，\n",
    "- 该代码会警告您 `ValueError: byte must be in range(0, 256)`。BPE 分词器通常使用这 256 个值作为其前 256 个单字符标记；您可以通过运行以下代码来直观地检查这一点：\n",
    "```python\n",
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")\n",
    "\"\"\"\n",
    "prints:\n",
    "0: !\n",
    "1: \"\n",
    "2: #\n",
    "...\n",
    "255: �  # <---- single character tokens up to here\n",
    "256:  t\n",
    "257:  a\n",
    "...\n",
    "298: ent\n",
    "299:  n\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd44b48-3347-4030-85eb-8c5e4862461a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T16:58:55.543563Z",
     "start_time": "2025-11-13T16:58:55.392279Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: !\n",
      "1: \"\n",
      "2: #\n",
      "3: $\n",
      "4: %\n",
      "5: &\n",
      "6: '\n",
      "7: (\n",
      "8: )\n",
      "9: *\n",
      "10: +\n",
      "11: ,\n",
      "12: -\n",
      "13: .\n",
      "14: /\n",
      "15: 0\n",
      "16: 1\n",
      "17: 2\n",
      "18: 3\n",
      "19: 4\n",
      "20: 5\n",
      "21: 6\n",
      "22: 7\n",
      "23: 8\n",
      "24: 9\n",
      "25: :\n",
      "26: ;\n",
      "27: <\n",
      "28: =\n",
      "29: >\n",
      "30: ?\n",
      "31: @\n",
      "32: A\n",
      "33: B\n",
      "34: C\n",
      "35: D\n",
      "36: E\n",
      "37: F\n",
      "38: G\n",
      "39: H\n",
      "40: I\n",
      "41: J\n",
      "42: K\n",
      "43: L\n",
      "44: M\n",
      "45: N\n",
      "46: O\n",
      "47: P\n",
      "48: Q\n",
      "49: R\n",
      "50: S\n",
      "51: T\n",
      "52: U\n",
      "53: V\n",
      "54: W\n",
      "55: X\n",
      "56: Y\n",
      "57: Z\n",
      "58: [\n",
      "59: \\\n",
      "60: ]\n",
      "61: ^\n",
      "62: _\n",
      "63: `\n",
      "64: a\n",
      "65: b\n",
      "66: c\n",
      "67: d\n",
      "68: e\n",
      "69: f\n",
      "70: g\n",
      "71: h\n",
      "72: i\n",
      "73: j\n",
      "74: k\n",
      "75: l\n",
      "76: m\n",
      "77: n\n",
      "78: o\n",
      "79: p\n",
      "80: q\n",
      "81: r\n",
      "82: s\n",
      "83: t\n",
      "84: u\n",
      "85: v\n",
      "86: w\n",
      "87: x\n",
      "88: y\n",
      "89: z\n",
      "90: {\n",
      "91: |\n",
      "92: }\n",
      "93: ~\n",
      "94: �\n",
      "95: �\n",
      "96: �\n",
      "97: �\n",
      "98: �\n",
      "99: �\n",
      "100: �\n",
      "101: �\n",
      "102: �\n",
      "103: �\n",
      "104: �\n",
      "105: �\n",
      "106: �\n",
      "107: �\n",
      "108: �\n",
      "109: �\n",
      "110: �\n",
      "111: �\n",
      "112: �\n",
      "113: �\n",
      "114: �\n",
      "115: �\n",
      "116: �\n",
      "117: �\n",
      "118: �\n",
      "119: �\n",
      "120: �\n",
      "121: �\n",
      "122: �\n",
      "123: �\n",
      "124: �\n",
      "125: �\n",
      "126: �\n",
      "127: �\n",
      "128: �\n",
      "129: �\n",
      "130: �\n",
      "131: �\n",
      "132: �\n",
      "133: �\n",
      "134: �\n",
      "135: �\n",
      "136: �\n",
      "137: �\n",
      "138: �\n",
      "139: �\n",
      "140: �\n",
      "141: �\n",
      "142: �\n",
      "143: �\n",
      "144: �\n",
      "145: �\n",
      "146: �\n",
      "147: �\n",
      "148: �\n",
      "149: �\n",
      "150: �\n",
      "151: �\n",
      "152: �\n",
      "153: �\n",
      "154: �\n",
      "155: �\n",
      "156: �\n",
      "157: �\n",
      "158: �\n",
      "159: �\n",
      "160: �\n",
      "161: �\n",
      "162: �\n",
      "163: �\n",
      "164: �\n",
      "165: �\n",
      "166: �\n",
      "167: �\n",
      "168: �\n",
      "169: �\n",
      "170: �\n",
      "171: �\n",
      "172: �\n",
      "173: �\n",
      "174: �\n",
      "175: �\n",
      "176: �\n",
      "177: �\n",
      "178: �\n",
      "179: �\n",
      "180: �\n",
      "181: �\n",
      "182: �\n",
      "183: �\n",
      "184: �\n",
      "185: �\n",
      "186: �\n",
      "187: �\n",
      "188: \u0000\n",
      "189: \u0001\n",
      "190: \u0002\n",
      "191: \u0003\n",
      "192: \u0004\n",
      "193: \u0005\n",
      "194: \u0006\n",
      "195: \u0007\n",
      "196:\n",
      "197: \t\n",
      "198: \n",
      "\n",
      "199: \u000b",
      "\n",
      "200: \f",
      "\n",
      "201: \r\n",
      "202: \u000e\n",
      "203: \u000f\n",
      "204: \u0010\n",
      "205: \u0011\n",
      "206: \u0012\n",
      "207: \u0013\n",
      "208: \u0014\n",
      "209: \u0015\n",
      "210: \u0016\n",
      "211: \u0017\n",
      "212: \u0018\n",
      "213: \u0019\n",
      "214: \u001a\n",
      "215: \u001b\n",
      "216: \u001c",
      "\n",
      "217: \u001d",
      "\n",
      "218: \u001e",
      "\n",
      "219: \u001f\n",
      "220:  \n",
      "221: \n",
      "222: �\n",
      "223: �\n",
      "224: �\n",
      "225: �\n",
      "226: �\n",
      "227: �\n",
      "228: �\n",
      "229: �\n",
      "230: �\n",
      "231: �\n",
      "232: �\n",
      "233: �\n",
      "234: �\n",
      "235: �\n",
      "236: �\n",
      "237: �\n",
      "238: �\n",
      "239: �\n",
      "240: �\n",
      "241: �\n",
      "242: �\n",
      "243: �\n",
      "244: �\n",
      "245: �\n",
      "246: �\n",
      "247: �\n",
      "248: �\n",
      "249: �\n",
      "250: �\n",
      "251: �\n",
      "252: �\n",
      "253: �\n",
      "254: �\n",
      "255: �\n",
      "256:  t\n",
      "257:  a\n",
      "258: he\n",
      "259: in\n",
      "260: re\n",
      "261: on\n",
      "262:  the\n",
      "263: er\n",
      "264:  s\n",
      "265: at\n",
      "266:  w\n",
      "267:  o\n",
      "268: en\n",
      "269:  c\n",
      "270: it\n",
      "271: is\n",
      "272: an\n",
      "273: or\n",
      "274: es\n",
      "275:  b\n",
      "276: ed\n",
      "277:  f\n",
      "278: ing\n",
      "279:  p\n",
      "280: ou\n",
      "281:  an\n",
      "282: al\n",
      "283: ar\n",
      "284:  to\n",
      "285:  m\n",
      "286:  of\n",
      "287:  in\n",
      "288:  d\n",
      "289:  h\n",
      "290:  and\n",
      "291: ic\n",
      "292: as\n",
      "293: le\n",
      "294:  th\n",
      "295: ion\n",
      "296: om\n",
      "297: ll\n",
      "298: ent\n",
      "299:  n\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "gpt2_tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(300):\n",
    "    decoded = gpt2_tokenizer.decode([i])\n",
    "    print(f\"{i}: {decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff0207-7f8e-44fa-9381-2a4bd83daab3",
   "metadata": {},
   "source": [
    "<!-- - Above, note that entries 256 and 257 are not single-character values but double-character values (a whitespace + a letter), which is a little shortcoming of the original GPT-2 BPE Tokenizer (this has been improved in the GPT-4 tokenizer)-->\n",
    "- 如上所述，请注意第 256 和 257 行不是单字符值，而是双字符值（一个空格 + 一个字母），这是原始 GPT-2 BPE 分词器的一个小缺陷（GPT-4 分词器已经改进了这一点）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8241c23a-d487-488d-bded-cdf054e24920",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.2 Building the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c2ceb7-0b3f-4a62-8dcc-07810cd8886e",
   "metadata": {},
   "source": [
    "- The goal of the BPE tokenization algorithm is to build a vocabulary of commonly occurring subwords like `298: ent` (which can be found in *entangle, entertain, enter, entrance, entity, ...*, for example), or even complete words like \n",
    "\n",
    "```\n",
    "318: is\n",
    "617: some\n",
    "1212: This\n",
    "2420: text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0d4420-a4c7-4813-916a-06f4f46bc3f0",
   "metadata": {},
   "source": [
    "- The BPE algorithm was originally described in 1994: \"[A New Algorithm for Data Compression](http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM)\" by Philip Gage\n",
    "- Before we get to the actual code implementation, the form that is used for LLM tokenizers today can be summarized as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc71db9-b070-48c4-8412-81f45b308ab3",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.3 BPE algorithm outline(BPE算法概述)\n",
    "\n",
    "**1. Identify frequent pairs(识别频繁配对)**\n",
    "- In each iteration, scan the text to find the most commonly occurring pair of bytes (or characters)\n",
    "- 在每次迭代中，扫描文本以找到出现频率最高的字节（或字符）对。\n",
    "\n",
    "**2. Replace and record(替换并记录)**\n",
    "\n",
    "- Replace that pair with a new placeholder ID (one not already in use, e.g., if we start with 0...255, the first placeholder would be 256)\n",
    "- 将该对替换为一个新的占位符 ID（尚未使用的 ID，例如，如果我们从 0...255 开始，则第一个占位符将是 256）。\n",
    "- Record this mapping in a lookup table\n",
    "- 在一个查找表中记录这个映射关系\n",
    "- The size of the lookup table is a hyperparameter, also called \"vocabulary size\" (for GPT-2, that's\n",
    "50,257)\n",
    "- 查找表的大小是一个超参数，也称为“词汇表大小”（对于 GPT-2，该值为：50,257）\n",
    "\n",
    "**3. Repeat until no gains(重复此步骤直至不再产生收益)**\n",
    "\n",
    "- Keep repeating steps 1 and 2, continually merging the most frequent pairs\n",
    "- 保持重复步骤1和步骤2，不断合并出现频率最高的配对\n",
    "- Stop when no further compression is possible (e.g., no pair occurs more than once)\n",
    "- 当无法进一步压缩时停止（例如，没有一对元素出现超过一次）。\n",
    "\n",
    "**Decompression (decoding)(解压缩（解码）)**\n",
    "\n",
    "- To restore the original text, reverse the process by substituting each ID with its corresponding pair, using the lookup table\n",
    "- 利用反向操作将使用查找表将每个 ID 替换为其对应的ID对来恢复原文。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f5ac9a-3528-4186-9468-8420c7b2ac00",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 1.4 BPE algorithm example (BPE算法示例)\n",
    "\n",
    "### 1.4.1 Concrete example of the encoding part (steps 1 & 2)(编码部分的具体示例（步骤 1 和 2）)\n",
    "\n",
    "- Suppose we have the text (training dataset) `the cat in the hat` from which we want to build the vocabulary for a BPE tokenizer\n",
    "- 假设我们有文本（训练数据集）“戴帽子的猫”，我们想用它构建 BPE 分词器的词汇表。\n",
    "\n",
    "**Iteration 1(迭代 1)**\n",
    "\n",
    "1. Identify frequent pairs(识别频繁配对)\n",
    "  - In this text, \"th\" appears twice (at the beginning and before the second \"e\")\n",
    "  - 在这段文字中，“th”出现了两次（一次在开头，一次在第二个“e”之前）。\n",
    "\n",
    "2. Replace and record(替换并记录)\n",
    "  - replace \"th\" with a new token ID that is not already in use, e.g., 256\n",
    "  - 将“th”替换为尚未使用的新令牌 ID，例如 256。\n",
    "  - the new text is: `<256>e cat in <256>e hat`\n",
    "  - 新文本为：`<256>e cat in <256>e hat`\n",
    "  - the new vocabulary is\n",
    "  - 新词汇是\n",
    "\n",
    "```\n",
    "  0: ...\n",
    "  ...\n",
    "  256: \"th\"\n",
    "```\n",
    "\n",
    "**Iteration 2(迭代2)**\n",
    "\n",
    "1. **Identify frequent pairs(识别频繁配对)**  \n",
    "   - In the text `<256>e cat in <256>e hat`, the pair `<256>e` appears twice\n",
    "   - 在文本 `<256>e cat in <256>e hat`中， `<256>e`这对数字出现了两次\n",
    "\n",
    "2. **Replace and record(替换并记录)**  \n",
    "   - replace `<256>e` with a new token ID that is not already in use, for example, `257`.\n",
    "   - 将`<256>e`替换为未使用的新tokenID，例如`257`\n",
    "   - The new text is:\n",
    "   - 新的文本是\n",
    "     ```\n",
    "     <257> cat in <257> hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "   - 更新后的词汇为\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     ```\n",
    "\n",
    "**Iteration 3(迭代3)**\n",
    "\n",
    "1. **Identify frequent pairs(识别频繁配对)**  \n",
    "   - In the text `<257> cat in <257> hat`, the pair `<257> ` appears twice (once at the beginning and once before “hat”).\n",
    "   - 在文本 `<257> cat in <257> hat`中， `<257> `这对符号出现了两次（一次在开头，一次在“hat”之前）。\n",
    "\n",
    "2. **Replace and record(替换并记录)**  \n",
    "   - replace `<257> ` with a new token ID that is not already in use, for example, `258`.\n",
    "   - 将 `<257>` 替换为尚未使用的新令牌 ID，例如 `258`。\n",
    "   - the new text is:\n",
    "   - 新文本是\n",
    "     ```\n",
    "     <258>cat in <258>hat\n",
    "     ```\n",
    "   - The updated vocabulary is:\n",
    "   - 更新后的词汇为\n",
    "     ```\n",
    "     0: ...\n",
    "     ...\n",
    "     256: \"th\"\n",
    "     257: \"<256>e\"\n",
    "     258: \"<257> \"\n",
    "     ```\n",
    "     \n",
    "- and so forth\n",
    "- 等等\n",
    "\n",
    "&nbsp;\n",
    "### 1.4.2 Concrete example of the decoding part (steps 3)(解码部分的具体示例（步骤 3）)\n",
    "\n",
    "- To restore the original text, we reverse the process by substituting each token ID with its corresponding pair in the reverse order they were introduced\n",
    "- 为了恢复原文，我们反向操作，将每个词元 ID 替换为其对应的词元对，替换顺序与它们最初出现的顺序相反。\n",
    "- Start with the final compressed text: `<258>cat in <258>hat`\n",
    "- 从最终的压缩文本开始：`<258>cat in <258>hat`\n",
    "-  Substitute `<258>` → `<257> `: `<257> cat in <257> hat`\n",
    "-  代替 `<258>` → `<257> `: `<257> cat in <257> hat`\n",
    "- Substitute `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "- 代替 `<257>` → `<256>e`: `<256>e cat in <256>e hat`\n",
    "- Substitute `<256>` → \"th\": `the cat in the hat`\n",
    "- 代替`<256>` → \"th\": `the cat in the hat`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2324948-ddd0-45d1-8ba8-e8eda9fc6677",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "## 2. A simple BPE implementation(一个简单的BPE实现)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ca709-40d7-4e3d-bf3e-4f5687a2e19b",
   "metadata": {},
   "source": [
    "<!-- - Below is an implementation of this algorithm described above as a Python class that mimics the `tiktoken` Python user interface\n",
    "- Note that the encoding part above describes the original training step via `train()`; however, the `encode()` method works similarly (although it looks a bit more complicated because of the special token handling):-->\n",
    "- 以下是上述算法的 Python 类实现，它模仿了 `tiktoken` Python 用户界面。\n",
    "- 请注意，上面的编码部分描述了通过 `train()` 进行的原始训练步骤；但是，`encode()` 方法的工作方式类似（尽管由于特殊的标记处理，它看起来稍微复杂一些）：\n",
    "\n",
    "<!-- 1. Split the input text into individual bytes\n",
    "2. Repeatedly find & replace (merge) adjacent tokens (pairs) when they match any pair in the learned BPE merges (from highest to lowest \"rank,\" i.e., in the order they were learned)\n",
    "3. Continue merging until no more merges can be applied\n",
    "4. The final list of token IDs is the encoded output -->\n",
    "1. 将输入文本分割成单个字节\n",
    "2. 重复查找并替换（合并）相邻的词元（词元对），只要它们与已学习的 BPE 合并结果中的任何词元对匹配即可（从最高“排名”到最低，即按照学习顺序）\n",
    "3. 继续合并，直到无法再进行合并为止\n",
    "4. 最终的词元 ID 列表即为编码后的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4a15ec-2667-4f56-b7c1-34e8071b621d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T17:23:33.811407Z",
     "start_time": "2025-11-13T17:23:33.797872Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "\n",
    "\n",
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # 将 token_id 映射到 token_str\n",
    "        self.vocab = {}\n",
    "        # 将 token_str 映射到 token_id\n",
    "        self.inverse_vocab = {}\n",
    "        # BPE 合并字典             ：{(token_id1, token_id2)        : merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        从头开始训练 BPE 分词器。\n",
    "        参数：\n",
    "            text（字符串）：训练文本。\n",
    "            vocab_size（整数）：所需的词汇表大小。\n",
    "            allowed_special（集合）：要包含的特殊词元集合。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 预处理：将空格替换为 'Ġ'\n",
    "        # 注意：Ġ 是 GPT-2 BPE 实现的特殊之处\n",
    "        # 例如，“Hello world” 可能被分词为 [\"Hello\", \"Ġworld\"]\n",
    "        # （GPT-4 BPE 会将其分词为 [\"Hello\", \" world\"]）\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # 使用唯一字符初始化词汇表，如果存在，则包含字母“Ġ”\n",
    "        # 从前 256 个 ASCII 字符开始\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "\n",
    "        # 将 processed_text 中尚未包含的字符添加到 unique_chars 中。\n",
    "        unique_chars.extend(char for char in sorted(set(processed_text)) if char not in unique_chars)\n",
    "\n",
    "        # （可选）如果与文本处理相关，请确保包含“Ġ”。\n",
    "        if 'Ġ' not in unique_chars:\n",
    "            unique_chars.append('Ġ')\n",
    "\n",
    "        # 现在创建词汇表和逆词汇表\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # 添加允许的特殊标记\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # 将处理后的文本分词为标记 ID\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE步骤1-3：反复查找并替换频繁配对\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:  #没有更多配对可以合并了。停止训练。\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # 使用合并的词元构建词汇表\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        将输入文本编码为标记 ID 列表。\n",
    "        参数：\n",
    "            text（字符串）：要编码的文本。\n",
    "        返回值：\n",
    "            List[int]：标记 ID 列表。\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        # 将文本分割成词元，并保留换行符。\n",
    "        words = text.replace(\"\\n\", \" \\n \").split()  #确保将换行符“\\n”视为单独的标记。\n",
    "\n",
    "        for i, word in enumerate(words):\n",
    "            if i > 0 and not word.startswith(\"\\n\"):\n",
    "                tokens.append(\"Ġ\" + word)  # 在空格或换行符后的单词前添加“Ġ”。\n",
    "            else:\n",
    "                tokens.append(word)  # 处理第一个单词或单独的换行符'\\n'\n",
    "\n",
    "        token_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                # token 包含在词汇表中。\n",
    "                token_id = self.inverse_vocab[token]\n",
    "                token_ids.append(token_id)\n",
    "            else:\n",
    "                # 尝试通过 BPE 处理子词分词。\n",
    "                sub_token_ids = self.tokenize_with_bpe(token)\n",
    "                token_ids.extend(sub_token_ids)\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "\n",
    "        使用 BPE 合并对单个词元进行分词。\n",
    "        参数：\n",
    "            token (str)：要进行分词的词元。\n",
    "        返回值：\n",
    "            List[int]：应用 BPE 后的词元 ID 列表。\n",
    "        \"\"\"\n",
    "        # 将标记分解成单个字符（作为初始标记 ID）\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        can_merge = True\n",
    "        while can_merge and len(token_ids) > 1:\n",
    "            can_merge = False\n",
    "            new_tokens = []\n",
    "            i = 0\n",
    "            while i < len(token_ids) - 1:\n",
    "                pair = (token_ids[i], token_ids[i + 1])\n",
    "                if pair in self.bpe_merges:\n",
    "                    merged_token_id = self.bpe_merges[pair]\n",
    "                    new_tokens.append(merged_token_id)\n",
    "                    # 取消注释仅供学习目的：\n",
    "                    print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                    i += 2  # 跳过下一个令牌，因为它已被合并。\n",
    "                    can_merge = True\n",
    "                else:\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                    i += 1\n",
    "            if i < len(token_ids):\n",
    "                new_tokens.append(token_ids[i])\n",
    "            token_ids = new_tokens\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        将令牌 ID 列表解码回字符串。\n",
    "\n",
    "        参数：\n",
    "            token_ids (List[int]): 要解码的令牌 ID 列表。\n",
    "\n",
    "        返回值：\n",
    "\n",
    "            str: 解码后的字符串。\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for token_id in token_ids:\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token.startswith(\"Ġ\"):\n",
    "                #  空格替换成'Ġ'\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46db7310-79c7-4ee0-b5fa-d760c6e1aa67",
   "metadata": {},
   "source": [
    "<!-- - There is a lot of code in the `BPETokenizerSimple` class above, and discussing it in detail is out of scope for this notebook, but the next section offers a short overview of the usage to understand the class methods a bit better -->\n",
    "上面的 `BPETokenizerSimple` 类包含大量代码，详细讨论超出了本笔记本的范围，但下一节将简要概述其用法，以便更好地理解类方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe1836-eed4-40dc-860b-2d23074d067e",
   "metadata": {},
   "source": [
    "## 3. BPE implementation walkthrough(BPE实施流程概述)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c996c-fd34-484f-a877-13d977214cf7",
   "metadata": {},
   "source": [
    "<!-- - In practice, I highly recommend using [tiktoken](https://github.com/openai/tiktoken) as my implementation above focuses on readability and educational purposes, not on performance-->\n",
    "- 实际上，我强烈建议使用 [tiktoken](https://github.com/openai/tiktoken)，因为我上面的实现侧重于可读性和教育目的，而非性能。\n",
    "<!-- - However, the usage is more or less similar to tiktoken, except that tiktoken does not have a training method -->\n",
    "- 然而，它的使用方法与 tiktoken 大体相似，只是 tiktoken 没有训练方法。\n",
    "<!-- - Let's see how my `BPETokenizerSimple` Python code above works by looking at some examples below (a detailed code discussion is out of scope for this notebook) -->\n",
    "- 让我们通过下面的一些示例来了解一下我上面的 `BPETokenizerSimple` Python 代码是如何工作的（详细的代码讨论超出了本笔记本的范围）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82acaf6-7ed5-4d3b-81c0-ae4d3559d2c7",
   "metadata": {},
   "source": [
    "### 3.1 Training, encoding, and decoding(训练，编码和解码)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962bf037-903e-4555-b09c-206e1a410278",
   "metadata": {},
   "source": [
    "<!-- - First, let's consider some sample text as our training dataset: -->\n",
    "- 首先，我们考虑一些示例文本作为训练数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d197cad-ed10-4a42-b01c-a763859781fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has the-verdict.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "if not os.path.exists(\"../01_main-chapter-code/the-verdict.txt\"):\n",
    "    print(\"not has the-verdict.txt\")\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"../01_main-chapter-code/the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "else:\n",
    "    print(\"has the-verdict.txt\")\n",
    "with open(\"../01_main-chapter-code/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f: # 添加 ../01_main-chapter-code/\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1b6ac-71d3-4817-956a-9bc7e463a84a",
   "metadata": {},
   "source": [
    "<!-- - Next, let's initialize and train the BPE tokenizer with a vocabulary size of 1,000 -->\n",
    "- 接下来，我们初始化并训练词汇量为 1000 的 BPE 分词器。\n",
    "<!-- - Note that the vocabulary size is already 255 by default due to the byte values discussed earlier, so we are only \"learning\" 745 vocabulary entries -->\n",
    "- 请注意，由于前面讨论的字节值，词汇表大小默认已为 255，因此我们实际上只“学习”了 745 个词汇条目。\n",
    "<!-- - For comparison, the GPT-2 vocabulary is 50,257 tokens, the GPT-4 vocabulary is 100,256 tokens (`cl100k_base` in tiktoken), and GPT-4o uses 199,997 tokens (`o200k_base` in tiktoken); they have all much bigger training sets compared to our simple example text above -->\n",
    "- 作为对比，GPT-2 的词汇表包含 50,257 个词元，GPT-4 的词汇表包含 100,256 个词元（在 tiktoken 中称为 `cl100k_base`），而 GPT-4o 则使用 199,997 个词元（在 tiktoken 中称为 `o200k_base`）；它们的训练集都比我们上面简单的示例文本要大得多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "027348fd-d52f-4396-93dd-38eed142df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2474ff05-5629-4f13-9e03-a47b1e713850",
   "metadata": {},
   "source": [
    "<!-- - You may want to inspect the vocabulary contents (but note it will create a long list) -->\n",
    "- 您可能需要查看词汇表内容（但请注意，这将生成一个很长的列表）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f705a283-355e-4460-b940-06bbc2ae4e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# print(tokenizer.vocab)\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9da0f-8a18-41cd-91ea-9ccc2bb5febb",
   "metadata": {},
   "source": [
    "<!-- - This vocabulary is created by merging 742 times (~ `1000 - len(range(0, 256))`) -->\n",
    "- 该词汇表是通过合并 742 次创建的(~ `1000 - len(range(0, 256))`)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3da42d1c-f75c-4ba7-a6c5-4cb8543d4a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac69c9-8413-482a-8148-6b2afbf1fb89",
   "metadata": {},
   "source": [
    "<!-- - This means that the first 256 entries are single-character tokens -->\n",
    "- 这意味着前 256 个条目都是单字符标记。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a4108-7c8b-4b98-9c67-d622e9cdf250",
   "metadata": {},
   "source": [
    "<!-- - Next, let's use the created merges via the `encode` method to encode some text: -->\n",
    "- 接下来，让我们使用 encode 方法对创建的合并结果进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1db5cce-e015-412b-ad56-060b8b638078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged pair (101, 109) -> 654 ('em')\n",
      "Merged pair (98, 114) -> 531 ('br')\n",
      "Merged pair (97, 99) -> 302 ('ac')\n",
      "Merged pair (101, 100) -> 311 ('ed')\n",
      "Merged pair (98, 101) -> 296 ('be')\n",
      "Merged pair (117, 116) -> 465 ('ut')\n",
      "Merged pair (256, 97) -> 287 ('Ġa')\n",
      "Merged pair (287, 114) -> 841 ('Ġar')\n",
      "Merged pair (256, 97) -> 287 ('Ġa')\n",
      "Merged pair (110, 100) -> 466 ('nd')\n",
      "Merged pair (108, 105) -> 326 ('li')\n",
      "Merged pair (102, 101) -> 972 ('fe')\n",
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Jack embraced beauty through art and life.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ed1b344-f7d4-4e9e-ac34-2a04b5c5b7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 42\n",
      "Number of token IDs: 20\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of characters:\", len(input_text))\n",
    "print(\"Number of token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1cfb9-402a-4e1e-9678-0b7547406248",
   "metadata": {},
   "source": [
    "<!-- - From the lengths above, we can see that a 42-character sentence was encoded into 20 token IDs, effectively cutting the input length roughly in half compared to a character-byte-based encoding -->\n",
    "- 从上述长度可以看出，一个 42 个字符的句子被编码成 20 个标记 ID，与基于字符-字节的编码相比，输入长度有效地减少了大约一半。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252693ee-e806-4dac-ab76-2c69086360f4",
   "metadata": {},
   "source": [
    "<!-- - Note that the vocabulary itself is used in the `decode()` method, which allows us to map the token IDs back into text: -->\n",
    "- 请注意，词汇表本身在 `decode()` 方法中使用，这使我们能够将标记 ID 映射回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da0e1faf-1933-43d9-b681-916c282a8f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[424, 256, 654, 531, 302, 311, 256, 296, 97, 465, 121, 595, 841, 116, 287, 466, 256, 326, 972, 46]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b690e83-5d6b-409a-804e-321c287c24a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jack embraced beauty through art and life.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adea5d09-e5ef-4721-994b-b9b25662fa0a",
   "metadata": {},
   "source": [
    "<!-- - Iterating over each token ID can give us a better understanding of how the token IDs are decoded via the vocabulary: -->\n",
    "- 遍历每个词元 ID 可以让我们更好地理解词元 ID 如何通过词汇表进行解码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b9e6289-92cb-4d88-b3c8-e836d7c8095f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424 -> Jack\n",
      "256 ->  \n",
      "654 -> em\n",
      "531 -> br\n",
      "302 -> ac\n",
      "311 -> ed\n",
      "256 ->  \n",
      "296 -> be\n",
      "97 -> a\n",
      "465 -> ut\n",
      "121 -> y\n",
      "595 ->  through\n",
      "841 ->  ar\n",
      "116 -> t\n",
      "287 ->  a\n",
      "466 -> nd\n",
      "256 ->  \n",
      "326 -> li\n",
      "972 -> fe\n",
      "46 -> .\n"
     ]
    }
   ],
   "source": [
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea41c6c-5538-4fd5-8b5f-195960853b71",
   "metadata": {},
   "source": [
    "<!-- - As we can see, most token IDs represent 2-character subwords; that's because the training data text is very short with not that many repetitive words, and because we used a relatively small vocabulary size -->\n",
    "- 我们可以看到，大多数词元 ID 代表的是双字符子词；这是因为训练数据文本很短，重复词不多，而且我们使用的词汇量也相对较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600055a3-7ec8-4abf-b88a-c4186fb71463",
   "metadata": {},
   "source": [
    "<!-- - As a summary, calling `decode(encode())` should be able to reproduce arbitrary input texts: -->\n",
    "- 总而言之，调用 `decode(encode())` 应该能够重现任意输入的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7056cb1-a9a3-4cf6-8364-29fb493ae240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged pair (84, 104) -> 542 ('Th')\n",
      "Merged pair (105, 115) -> 299 ('is')\n",
      "Merged pair (105, 115) -> 299 ('is')\n",
      "Merged pair (256, 115) -> 321 ('Ġs')\n",
      "Merged pair (111, 109) -> 305 ('om')\n",
      "Merged pair (256, 116) -> 259 ('Ġt')\n",
      "Merged pair (101, 120) -> 461 ('ex')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"This is some text.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558af04-483c-4f6b-88f5-a534f37316cd",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "# 4. Conclusion 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ed0e6-ad06-4bb3-bb39-6b8110c1caa4",
   "metadata": {},
   "source": [
    "<!-- - That's it! That's how BPE works in a nutshell, complete with a training method for creating new tokenizers -->\n",
    "- 就是这样！这就是BPE的简要工作原理，其中包含了创建新分词器的训练方法。\n",
    "<!-- - I hope you found this brief tutorial useful for educational purposes; if you have any questions, please feel free to open a new Discussion [here](https://github.com/rasbt/LLMs-from-scratch/discussions/categories/q-a) -->\n",
    "\n",
    "\n",
    "<!-- **This is a very naive implementation for educational purposes. The [bpe-from-scratch.ipynb](bpe-from-scratch.ipynb) notebook contains a more sophisticated (but much harder to read) implementation that matches the behavior in tiktoken.** -->\n",
    "**这是一个非常简单的教学用途实现。[bpe-from-scratch.ipynb](bpe-from-scratch.ipynb) notebook 中包含一个更复杂（但更难读懂）的实现，它与 tiktoken 的行为相匹配**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
